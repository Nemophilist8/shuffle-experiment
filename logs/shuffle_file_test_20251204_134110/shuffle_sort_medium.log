=== Dispatcher: Routing to task [file] ===
Running File Count Monitor with args: sort medium
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/12/04 13:41:12 INFO SparkContext: Running Spark version 1.6.3
25/12/04 13:41:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/12/04 13:41:13 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
25/12/04 13:41:13 WARN Utils: Your hostname, hygon7490 resolves to a loopback address: 127.0.0.1; using 49.52.27.65 instead (on interface ens2f1np1)
25/12/04 13:41:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/12/04 13:41:13 INFO SecurityManager: Changing view acls to: djk
25/12/04 13:41:13 INFO SecurityManager: Changing modify acls to: djk
25/12/04 13:41:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(djk); users with modify permissions: Set(djk)
25/12/04 13:41:13 INFO Utils: Successfully started service 'sparkDriver' on port 38291.
25/12/04 13:41:13 INFO Slf4jLogger: Slf4jLogger started
25/12/04 13:41:13 INFO Remoting: Starting remoting
25/12/04 13:41:13 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@49.52.27.65:32841]
25/12/04 13:41:13 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 32841.
25/12/04 13:41:13 INFO SparkEnv: Registering MapOutputTracker
25/12/04 13:41:13 INFO SparkEnv: Registering BlockManagerMaster
25/12/04 13:41:13 INFO DiskBlockManager: Created local directory at /tmp/spark/work/blockmgr-c2b6b45a-8785-4b3c-af2c-51b3e18e78e8
25/12/04 13:41:13 INFO MemoryStore: MemoryStore started with capacity 143.3 MB
25/12/04 13:41:13 INFO SparkEnv: Registering OutputCommitCoordinator
25/12/04 13:41:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/12/04 13:41:13 INFO SparkUI: Started SparkUI at http://49.52.27.65:4040
25/12/04 13:41:13 INFO HttpFileServer: HTTP File server directory is /tmp/spark/work/spark-d52db3b1-237e-4096-acdb-87027205682d/httpd-6be90bfc-4a95-47d3-b58a-ee6989a6ff56
25/12/04 13:41:13 INFO HttpServer: Starting HTTP Server
25/12/04 13:41:14 INFO Utils: Successfully started service 'HTTP file server' on port 41787.
25/12/04 13:41:14 INFO SparkContext: Added JAR file:/home/djk/Documents/shuffle-experiment/target/scala-2.10/spark-shuffle-experiment-1.0.0.jar at http://49.52.27.65:41787/jars/spark-shuffle-experiment-1.0.0.jar with timestamp 1764855674273
25/12/04 13:41:14 INFO AppClient$ClientEndpoint: Connecting to master spark://49.52.27.113:7077...
25/12/04 13:41:24 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20251204084114-0143
25/12/04 13:41:24 INFO AppClient$ClientEndpoint: Executor added: app-20251204084114-0143/0 on worker-20251203224515-49.52.27.60-35123 (49.52.27.60:35123) with 2 cores
25/12/04 13:41:24 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204084114-0143/0 on hostPort 49.52.27.60:35123 with 2 cores, 1024.0 MB RAM
25/12/04 13:41:24 INFO AppClient$ClientEndpoint: Executor added: app-20251204084114-0143/1 on worker-20251203144508-49.52.27.65-34725 (49.52.27.65:34725) with 2 cores
25/12/04 13:41:24 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204084114-0143/1 on hostPort 49.52.27.65:34725 with 2 cores, 1024.0 MB RAM
25/12/04 13:41:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41177.
25/12/04 13:41:24 INFO NettyBlockTransferService: Server created on 41177
25/12/04 13:41:24 INFO BlockManagerMaster: Trying to register BlockManager
25/12/04 13:41:24 INFO BlockManagerMasterEndpoint: Registering block manager 49.52.27.65:41177 with 143.3 MB RAM, BlockManagerId(driver, 49.52.27.65, 41177)
25/12/04 13:41:24 INFO BlockManagerMaster: Registered BlockManager
25/12/04 13:41:24 INFO AppClient$ClientEndpoint: Executor updated: app-20251204084114-0143/1 is now RUNNING
25/12/04 13:41:24 INFO AppClient$ClientEndpoint: Executor updated: app-20251204084114-0143/0 is now RUNNING
25/12/04 13:41:34 INFO EventLoggingListener: Logging events to file:/tmp/spark-events/app-20251204084114-0143
25/12/04 13:41:34 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
=== Shuffle File Monitor 实验 ===
3. Spark版本: 1.6.3
默认shuffle分区数: 200
请求的Shuffle Manager: sort
当前 Shuffle Manager: sort
Spark 本地目录: /tmp/spark/work
监控将每秒检查一次 /tmp/spark/work 目录中的 shuffle_* 文件
测试数据集: medium
分区数: 100
>>> 正在进行数据预热 (Cache & Count)...
>>> 预热完成! 数据已驻留内存。耗时: 28506ms, 记录数: 5000000
>>> 现在开始正式 Shuffle 实验 (此时文件应该会立即产生)

=== 开始运行 sort 实验 ===
启动Shuffle文件监控，每秒检查一次...
[ShuffleMonitor 13:42:04] 文件数: 92, 总大小: 1KB (峰值: 92文件, 1KB)
[ShuffleMonitor 13:42:05] 文件数: 92, 总大小: 1KB (峰值: 92文件, 1KB)
[ShuffleMonitor 13:42:06] 文件数: 96, 总大小: 147.53MB (峰值: 96文件, 147.53MB)
[ShuffleMonitor 13:42:07] 文件数: 96, 总大小: 147.53MB (峰值: 96文件, 147.53MB)
[ShuffleMonitor 13:42:08] 文件数: 99, 总大小: 294.69MB (峰值: 99文件, 294.69MB)
[ShuffleMonitor 13:42:09] 文件数: 102, 总大小: 363.92MB (峰值: 102文件, 363.92MB)
[ShuffleMonitor 13:42:10] 文件数: 104, 总大小: 443.68MB (峰值: 104文件, 443.68MB)
[ShuffleMonitor 13:42:11] 文件数: 108, 总大小: 591.56MB (峰值: 108文件, 591.56MB)
[ShuffleMonitor 13:42:12] 文件数: 109, 总大小: 596.41MB (峰值: 109文件, 596.41MB)
[ShuffleMonitor 13:42:13] 文件数: 112, 总大小: 739.78MB (峰值: 112文件, 739.78MB)
[ShuffleMonitor 13:42:14] 文件数: 116, 总大小: 887.59MB (峰值: 116文件, 887.59MB)
[ShuffleMonitor 13:42:15] 文件数: 120, 总大小: 1.01GB (峰值: 120文件, 1.01GB)
[ShuffleMonitor 13:42:16] 文件数: 121, 总大小: 1.01GB (峰值: 121文件, 1.01GB)
[ShuffleMonitor 13:42:17] 文件数: 124, 总大小: 1.16GB (峰值: 124文件, 1.16GB)
[ShuffleMonitor 13:42:18] 文件数: 127, 总大小: 1.29GB (峰值: 127文件, 1.29GB)
[ShuffleMonitor 13:42:19] 文件数: 127, 总大小: 1.30GB (峰值: 127文件, 1.30GB)
[ShuffleMonitor 13:42:20] 文件数: 131, 总大小: 1.43GB (峰值: 131文件, 1.43GB)
[ShuffleMonitor 13:42:21] 文件数: 132, 总大小: 1.45GB (峰值: 132文件, 1.45GB)
[ShuffleMonitor 13:42:22] 文件数: 136, 总大小: 1.59GB (峰值: 136文件, 1.59GB)
[ShuffleMonitor 13:42:23] 文件数: 138, 总大小: 1.62GB (峰值: 138文件, 1.62GB)
[ShuffleMonitor 13:42:24] 文件数: 139, 总大小: 1.72GB (峰值: 139文件, 1.72GB)
[ShuffleMonitor 13:42:25] 文件数: 141, 总大小: 1.75GB (峰值: 141文件, 1.75GB)
[ShuffleMonitor 13:42:26] 文件数: 142, 总大小: 1.86GB (峰值: 142文件, 1.86GB)
[ShuffleMonitor 13:42:27] 文件数: 145, 总大小: 1.90GB (峰值: 145文件, 1.90GB)
[ShuffleMonitor 13:42:28] 文件数: 146, 总大小: 1.90GB (峰值: 146文件, 1.90GB)
[ShuffleMonitor 13:42:29] 文件数: 148, 总大小: 2.02GB (峰值: 148文件, 2.02GB)
[ShuffleMonitor 13:42:30] 文件数: 149, 总大小: 2.05GB (峰值: 149文件, 2.05GB)
[ShuffleMonitor 13:42:31] 文件数: 152, 总大小: 2.17GB (峰值: 152文件, 2.17GB)
[ShuffleMonitor 13:42:32] 文件数: 155, 总大小: 2.28GB (峰值: 155文件, 2.28GB)
[ShuffleMonitor 13:42:33] 文件数: 157, 总大小: 2.32GB (峰值: 157文件, 2.32GB)
[ShuffleMonitor 13:42:34] 文件数: 160, 总大小: 2.46GB (峰值: 160文件, 2.46GB)
[ShuffleMonitor 13:42:35] 文件数: 161, 总大小: 2.49GB (峰值: 161文件, 2.49GB)
[ShuffleMonitor 13:42:36] 文件数: 164, 总大小: 2.60GB (峰值: 164文件, 2.60GB)
[ShuffleMonitor 13:42:37] 文件数: 167, 总大小: 2.68GB (峰值: 167文件, 2.68GB)
[ShuffleMonitor 13:42:38] 文件数: 167, 总大小: 2.72GB (峰值: 167文件, 2.72GB)
[ShuffleMonitor 13:42:39] 文件数: 170, 总大小: 2.82GB (峰值: 170文件, 2.82GB)
[ShuffleMonitor 13:42:40] 文件数: 173, 总大小: 2.92GB (峰值: 173文件, 2.92GB)
[ShuffleMonitor 13:42:41] 文件数: 173, 总大小: 2.96GB (峰值: 173文件, 2.96GB)
[ShuffleMonitor 13:42:42] 文件数: 177, 总大小: 3.07GB (峰值: 177文件, 3.07GB)
[ShuffleMonitor 13:42:43] 文件数: 178, 总大小: 3.11GB (峰值: 178文件, 3.11GB)
[ShuffleMonitor 13:42:44] 文件数: 181, 总大小: 3.21GB (峰值: 181文件, 3.21GB)
[ShuffleMonitor 13:42:45] 文件数: 183, 总大小: 3.25GB (峰值: 183文件, 3.25GB)
[ShuffleMonitor 13:42:46] 文件数: 183, 总大小: 3.26GB (峰值: 183文件, 3.26GB)
[ShuffleMonitor 13:42:47] 文件数: 186, 总大小: 3.40GB (峰值: 186文件, 3.40GB)
[ShuffleMonitor 13:42:48] 文件数: 187, 总大小: 3.40GB (峰值: 187文件, 3.40GB)
[ShuffleMonitor 13:42:49] 文件数: 190, 总大小: 3.54GB (峰值: 190文件, 3.54GB)
[ShuffleMonitor 13:42:50] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:42:51] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:42:52] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:42:53] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:42:54] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:42:55] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:42:56] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:42:57] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:42:58] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:42:59] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:00] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:01] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:02] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:03] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:04] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:05] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:06] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:07] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:08] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:09] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:10] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:11] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:12] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:13] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:14] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:15] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:16] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:17] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:18] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:19] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:20] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:21] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:22] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:23] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:24] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:25] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:26] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:27] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:28] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:29] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:30] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:31] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:32] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:33] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:34] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:35] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:36] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:37] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:38] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:39] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:40] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:41] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:42] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:43] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:44] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:45] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:46] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:47] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:49] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:50] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:51] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:52] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:53] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:54] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:55] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:56] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:57] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:58] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:43:59] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:44:00] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:44:01] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:44:02] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
25/12/04 13:44:02 WARN TaskSetManager: Lost task 435.0 in stage 3.0 (TID 636, 49.52.27.60): java.lang.OutOfMemoryError: GC overhead limit exceeded

25/12/04 13:44:02 WARN TaskSetManager: Lost task 435.1 in stage 3.0 (TID 701, 49.52.27.60): FetchFailed(BlockManagerId(0, 49.52.27.60, 43819), shuffleId=1, mapId=1, reduceId=435, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-289546f2-5e4f-49a0-bdbe-8dccd62ee01d/blockmgr-66e01fca-4f47-43eb-bd7e-8732938a6ab3/32/shuffle_1_1_0.index (No such file or directory)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-289546f2-5e4f-49a0-bdbe-8dccd62ee01d/blockmgr-66e01fca-4f47-43eb-bd7e-8732938a6ab3/32/shuffle_1_1_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:197)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:298)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:238)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:112)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:43)
	... 9 more

)
[ShuffleMonitor 13:44:03] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:44:04] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
25/12/04 13:44:04 WARN TaskSetManager: Lost task 437.0 in stage 3.0 (TID 638, 49.52.27.65): FetchFailed(BlockManagerId(0, 49.52.27.60, 43819), shuffleId=1, mapId=25, reduceId=437, message=
org.apache.spark.shuffle.FetchFailedException: java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-289546f2-5e4f-49a0-bdbe-8dccd62ee01d/blockmgr-66e01fca-4f47-43eb-bd7e-8732938a6ab3/14/shuffle_1_25_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:197)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:298)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:58)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:58)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:58)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:149)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:750)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-289546f2-5e4f-49a0-bdbe-8dccd62ee01d/blockmgr-66e01fca-4f47-43eb-bd7e-8732938a6ab3/14/shuffle_1_25_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:197)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:298)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:58)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:58)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:58)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:149)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:750)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:186)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:106)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
25/12/04 13:44:04 ERROR TaskSchedulerImpl: Lost executor 0 on 49.52.27.60: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:44:04 WARN TaskSetManager: Lost task 0.0 in stage 2.1 (TID 702, 49.52.27.60): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:44:04 WARN TaskSetManager: Lost task 1.0 in stage 2.1 (TID 703, 49.52.27.60): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
[ShuffleMonitor 13:44:05] 文件数: 192, 总大小: 3.62GB (峰值: 192文件, 3.62GB)
[ShuffleMonitor 13:44:06] 文件数: 194, 总大小: 3.69GB (峰值: 194文件, 3.69GB)
[ShuffleMonitor 13:44:07] 文件数: 195, 总大小: 3.69GB (峰值: 195文件, 3.69GB)
[ShuffleMonitor 13:44:08] 文件数: 196, 总大小: 3.76GB (峰值: 196文件, 3.76GB)
[ShuffleMonitor 13:44:09] 文件数: 198, 总大小: 3.83GB (峰值: 198文件, 3.83GB)
[ShuffleMonitor 13:44:10] 文件数: 199, 总大小: 3.84GB (峰值: 199文件, 3.84GB)
[ShuffleMonitor 13:44:11] 文件数: 201, 总大小: 3.91GB (峰值: 201文件, 3.91GB)
[ShuffleMonitor 13:44:12] 文件数: 203, 总大小: 3.98GB (峰值: 203文件, 3.98GB)
[ShuffleMonitor 13:44:13] 文件数: 204, 总大小: 4.05GB (峰值: 204文件, 4.05GB)
[ShuffleMonitor 13:44:14] 文件数: 206, 总大小: 4.12GB (峰值: 206文件, 4.12GB)
[ShuffleMonitor 13:44:15] 文件数: 208, 总大小: 4.20GB (峰值: 208文件, 4.20GB)
[ShuffleMonitor 13:44:16] 文件数: 210, 总大小: 4.27GB (峰值: 210文件, 4.27GB)
[ShuffleMonitor 13:44:17] 文件数: 211, 总大小: 4.29GB (峰值: 211文件, 4.29GB)
[ShuffleMonitor 13:44:18] 文件数: 213, 总大小: 4.36GB (峰值: 213文件, 4.36GB)
[ShuffleMonitor 13:44:19] 文件数: 214, 总大小: 4.41GB (峰值: 214文件, 4.41GB)
25/12/04 13:44:19 WARN TaskSetManager: Lost task 436.0 in stage 3.0 (TID 637, 49.52.27.65): FetchFailed(BlockManagerId(0, 49.52.27.60, 43819), shuffleId=1, mapId=69, reduceId=436, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:43819
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:43819
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:43819
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
[ShuffleMonitor 13:44:20] 文件数: 215, 总大小: 4.48GB (峰值: 215文件, 4.48GB)
[ShuffleMonitor 13:44:21] 文件数: 217, 总大小: 4.52GB (峰值: 217文件, 4.52GB)
[ShuffleMonitor 13:44:22] 文件数: 219, 总大小: 4.62GB (峰值: 219文件, 4.62GB)
[ShuffleMonitor 13:44:23] 文件数: 222, 总大小: 4.69GB (峰值: 222文件, 4.69GB)
[ShuffleMonitor 13:44:24] 文件数: 223, 总大小: 4.76GB (峰值: 223文件, 4.76GB)
[ShuffleMonitor 13:44:25] 文件数: 225, 总大小: 4.83GB (峰值: 225文件, 4.83GB)
[ShuffleMonitor 13:44:26] 文件数: 227, 总大小: 4.91GB (峰值: 227文件, 4.91GB)
[ShuffleMonitor 13:44:27] 文件数: 231, 总大小: 5.03GB (峰值: 231文件, 5.03GB)
[ShuffleMonitor 13:44:28] 文件数: 231, 总大小: 5.04GB (峰值: 231文件, 5.04GB)
[ShuffleMonitor 13:44:29] 文件数: 235, 总大小: 5.16GB (峰值: 235文件, 5.16GB)
[ShuffleMonitor 13:44:30] 文件数: 237, 总大小: 5.23GB (峰值: 237文件, 5.23GB)
[ShuffleMonitor 13:44:31] 文件数: 239, 总大小: 5.30GB (峰值: 239文件, 5.30GB)
[ShuffleMonitor 13:44:32] 文件数: 241, 总大小: 5.39GB (峰值: 241文件, 5.39GB)
[ShuffleMonitor 13:44:33] 文件数: 242, 总大小: 5.41GB (峰值: 242文件, 5.41GB)
[ShuffleMonitor 13:44:34] 文件数: 245, 总大小: 5.53GB (峰值: 245文件, 5.53GB)
[ShuffleMonitor 13:44:35] 文件数: 248, 总大小: 5.64GB (峰值: 248文件, 5.64GB)
[ShuffleMonitor 13:44:36] 文件数: 251, 总大小: 5.73GB (峰值: 251文件, 5.73GB)
[ShuffleMonitor 13:44:37] 文件数: 253, 总大小: 5.81GB (峰值: 253文件, 5.81GB)
[ShuffleMonitor 13:44:38] 文件数: 253, 总大小: 5.84GB (峰值: 253文件, 5.84GB)
[ShuffleMonitor 13:44:39] 文件数: 257, 总大小: 5.95GB (峰值: 257文件, 5.95GB)
[ShuffleMonitor 13:44:40] 文件数: 260, 总大小: 6.08GB (峰值: 260文件, 6.08GB)
[ShuffleMonitor 13:44:41] 文件数: 261, 总大小: 6.09GB (峰值: 261文件, 6.09GB)
[ShuffleMonitor 13:44:42] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:43] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:44] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:45] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:46] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:47] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:48] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:49] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:50] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:51] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:52] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:53] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:54] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:55] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:56] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:57] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:58] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:44:59] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:45:00] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:45:01] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:45:02] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
[ShuffleMonitor 13:45:03] 文件数: 264, 总大小: 6.22GB (峰值: 264文件, 6.22GB)
25/12/04 13:45:09 ERROR TaskSchedulerImpl: Lost executor 2 on 49.52.27.60: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:45:09 WARN TaskSetManager: Lost task 2.0 in stage 3.1 (TID 756, 49.52.27.60): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:45:34 WARN TaskSetManager: Lost task 2.1 in stage 3.1 (TID 757, 49.52.27.60): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=437, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:548)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:544)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:544)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:155)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

)
25/12/04 13:45:37 WARN TaskSetManager: Lost task 0.0 in stage 3.1 (TID 754, 49.52.27.65): FetchFailed(BlockManagerId(2, 49.52.27.60, 45133), shuffleId=1, mapId=84, reduceId=435, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:45133
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:45133
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:45133
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
25/12/04 13:45:39 WARN TaskSetManager: Lost task 1.0 in stage 3.1 (TID 755, 49.52.27.65): FetchFailed(BlockManagerId(2, 49.52.27.60, 45133), shuffleId=1, mapId=91, reduceId=436, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:45133
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:45133
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:45133
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
25/12/04 13:46:14 WARN TaskSetManager: Lost task 2.0 in stage 3.2 (TID 774, 49.52.27.60): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.Arrays.copyOfRange(Arrays.java:3664)
	at java.lang.String.<init>(String.java:207)
	at com.esotericsoftware.kryo.io.Input.readString(Input.java:448)
	at com.esotericsoftware.kryo.serializers.DefaultSerializers$StringSerializer.read(DefaultSerializers.java:157)
	at com.esotericsoftware.kryo.serializers.DefaultSerializers$StringSerializer.read(DefaultSerializers.java:146)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:228)
	at org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:171)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:201)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:198)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 13:46:14 WARN TaskSetManager: Lost task 2.1 in stage 3.2 (TID 775, 49.52.27.60): FetchFailed(BlockManagerId(3, 49.52.27.60, 42391), shuffleId=1, mapId=44, reduceId=437, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-289546f2-5e4f-49a0-bdbe-8dccd62ee01d/blockmgr-59e88696-4dba-4e2d-974a-a2ad7ba17e04/29/shuffle_1_44_0.index (No such file or directory)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-289546f2-5e4f-49a0-bdbe-8dccd62ee01d/blockmgr-59e88696-4dba-4e2d-974a-a2ad7ba17e04/29/shuffle_1_44_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:197)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:298)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:238)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:112)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:43)
	... 9 more

)
25/12/04 13:46:15 WARN TransportChannelHandler: Exception in connection from 49.52.27.60/49.52.27.60:46000
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:313)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:881)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:242)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:750)
25/12/04 13:46:15 ERROR TaskSchedulerImpl: Lost executor 3 on 49.52.27.60: Command exited with code 52
25/12/04 13:46:15 WARN TaskSetManager: Lost task 1.0 in stage 2.3 (TID 777, 49.52.27.60): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 52
25/12/04 13:46:15 WARN TaskSetManager: Lost task 0.0 in stage 2.3 (TID 776, 49.52.27.60): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Command exited with code 52
25/12/04 13:46:56 WARN TaskSetManager: Lost task 0.0 in stage 3.2 (TID 772, 49.52.27.65): FetchFailed(BlockManagerId(3, 49.52.27.60, 42391), shuffleId=1, mapId=56, reduceId=435, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:42391
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:42391
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:42391
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
25/12/04 13:46:57 WARN TaskSetManager: Lost task 1.0 in stage 3.2 (TID 773, 49.52.27.65): FetchFailed(BlockManagerId(3, 49.52.27.60, 42391), shuffleId=1, mapId=97, reduceId=436, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:42391
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:42391
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:42391
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
