=== Dispatcher: Routing to task [file] ===
Running File Count Monitor with args: sort medium
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/12/04 14:00:51 INFO SparkContext: Running Spark version 1.6.3
25/12/04 14:00:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/12/04 14:00:51 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
25/12/04 14:00:51 WARN Utils: Your hostname, hygon7490 resolves to a loopback address: 127.0.0.1; using 49.52.27.65 instead (on interface ens2f1np1)
25/12/04 14:00:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/12/04 14:00:51 INFO SecurityManager: Changing view acls to: djk
25/12/04 14:00:51 INFO SecurityManager: Changing modify acls to: djk
25/12/04 14:00:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(djk); users with modify permissions: Set(djk)
25/12/04 14:00:52 INFO Utils: Successfully started service 'sparkDriver' on port 37037.
25/12/04 14:00:52 INFO Slf4jLogger: Slf4jLogger started
25/12/04 14:00:52 INFO Remoting: Starting remoting
25/12/04 14:00:52 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@49.52.27.65:44841]
25/12/04 14:00:52 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 44841.
25/12/04 14:00:52 INFO SparkEnv: Registering MapOutputTracker
25/12/04 14:00:52 INFO SparkEnv: Registering BlockManagerMaster
25/12/04 14:00:52 INFO DiskBlockManager: Created local directory at /tmp/spark/work/blockmgr-2f53a1af-29ac-4763-9874-d04a07f15471
25/12/04 14:00:52 INFO MemoryStore: MemoryStore started with capacity 143.3 MB
25/12/04 14:00:52 INFO SparkEnv: Registering OutputCommitCoordinator
25/12/04 14:00:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/12/04 14:00:52 INFO SparkUI: Started SparkUI at http://49.52.27.65:4040
25/12/04 14:00:52 INFO HttpFileServer: HTTP File server directory is /tmp/spark/work/spark-ce32419b-32b4-48a6-a7fc-6eba65f18326/httpd-40555ac9-3700-441d-83a1-0d51c4a85878
25/12/04 14:00:52 INFO HttpServer: Starting HTTP Server
25/12/04 14:00:52 INFO Utils: Successfully started service 'HTTP file server' on port 37009.
25/12/04 14:00:53 INFO SparkContext: Added JAR file:/home/djk/Documents/shuffle-experiment/target/scala-2.10/spark-shuffle-experiment-1.0.0.jar at http://49.52.27.65:37009/jars/spark-shuffle-experiment-1.0.0.jar with timestamp 1764856853128
25/12/04 14:00:53 INFO AppClient$ClientEndpoint: Connecting to master spark://49.52.27.113:7077...
25/12/04 14:00:58 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20251204090053-0147
25/12/04 14:00:58 INFO AppClient$ClientEndpoint: Executor added: app-20251204090053-0147/0 on worker-20251203224515-49.52.27.60-35123 (49.52.27.60:35123) with 2 cores
25/12/04 14:00:58 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204090053-0147/0 on hostPort 49.52.27.60:35123 with 2 cores, 4.0 GB RAM
25/12/04 14:00:58 INFO AppClient$ClientEndpoint: Executor added: app-20251204090053-0147/1 on worker-20251203144508-49.52.27.65-34725 (49.52.27.65:34725) with 2 cores
25/12/04 14:00:58 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204090053-0147/1 on hostPort 49.52.27.65:34725 with 2 cores, 4.0 GB RAM
25/12/04 14:00:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34411.
25/12/04 14:00:58 INFO NettyBlockTransferService: Server created on 34411
25/12/04 14:00:58 INFO BlockManagerMaster: Trying to register BlockManager
25/12/04 14:00:58 INFO BlockManagerMasterEndpoint: Registering block manager 49.52.27.65:34411 with 143.3 MB RAM, BlockManagerId(driver, 49.52.27.65, 34411)
25/12/04 14:00:58 INFO BlockManagerMaster: Registered BlockManager
25/12/04 14:00:58 INFO AppClient$ClientEndpoint: Executor updated: app-20251204090053-0147/1 is now RUNNING
25/12/04 14:00:58 INFO AppClient$ClientEndpoint: Executor updated: app-20251204090053-0147/0 is now RUNNING
25/12/04 14:01:09 INFO EventLoggingListener: Logging events to file:/tmp/spark-events/app-20251204090053-0147
25/12/04 14:01:09 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
=== Shuffle File Monitor 实验 ===
3. Spark版本: 1.6.3
默认shuffle分区数: 200
请求的Shuffle Manager: sort
当前 Shuffle Manager: sort
Spark 本地目录: /tmp/spark/work
监控将每秒检查一次 /tmp/spark/work 目录中的 shuffle_* 文件
测试数据集: medium
分区数: 100
>>> 正在进行数据预热 (Cache & Count)...
>>> 预热完成! 数据已驻留内存。耗时: 32158ms, 记录数: 5000000
>>> 现在开始正式 Shuffle 实验 (此时文件应该会立即产生)

=== 开始运行 sort 实验 ===
启动Shuffle文件监控，每秒检查一次...
[ShuffleMonitor 14:01:42] 文件数: 102, 总大小: 1KB (峰值: 102文件, 1KB)
[ShuffleMonitor 14:01:43] 文件数: 104, 总大小: 59.92MB (峰值: 104文件, 59.92MB)
[ShuffleMonitor 14:01:44] 文件数: 108, 总大小: 273.49MB (峰值: 108文件, 273.49MB)
[ShuffleMonitor 14:01:45] 文件数: 114, 总大小: 444.38MB (峰值: 114文件, 444.38MB)
[ShuffleMonitor 14:01:46] 文件数: 118, 总大小: 592.24MB (峰值: 118文件, 592.24MB)
[ShuffleMonitor 14:01:47] 文件数: 123, 总大小: 743.26MB (峰值: 123文件, 743.26MB)
[ShuffleMonitor 14:01:48] 文件数: 127, 总大小: 901.72MB (峰值: 127文件, 901.72MB)
[ShuffleMonitor 14:01:49] 文件数: 132, 总大小: 1.11GB (峰值: 132文件, 1.11GB)
[ShuffleMonitor 14:01:50] 文件数: 136, 总大小: 1.25GB (峰值: 136文件, 1.25GB)
[ShuffleMonitor 14:01:51] 文件数: 141, 总大小: 1.42GB (峰值: 141文件, 1.42GB)
[ShuffleMonitor 14:01:52] 文件数: 145, 总大小: 1.55GB (峰值: 145文件, 1.55GB)
[ShuffleMonitor 14:01:53] 文件数: 148, 总大小: 1.68GB (峰值: 148文件, 1.68GB)
[ShuffleMonitor 14:01:54] 文件数: 152, 总大小: 1.83GB (峰值: 152文件, 1.83GB)
[ShuffleMonitor 14:01:55] 文件数: 156, 总大小: 1.93GB (峰值: 156文件, 1.93GB)
[ShuffleMonitor 14:01:56] 文件数: 161, 总大小: 2.11GB (峰值: 161文件, 2.11GB)
[ShuffleMonitor 14:01:57] 文件数: 163, 总大小: 2.24GB (峰值: 163文件, 2.24GB)
[ShuffleMonitor 14:01:58] 文件数: 168, 总大小: 2.39GB (峰值: 168文件, 2.39GB)
[ShuffleMonitor 14:01:59] 文件数: 172, 总大小: 2.53GB (峰值: 172文件, 2.53GB)
[ShuffleMonitor 14:02:00] 文件数: 176, 总大小: 2.68GB (峰值: 176文件, 2.68GB)
[ShuffleMonitor 14:02:01] 文件数: 180, 总大小: 2.82GB (峰值: 180文件, 2.82GB)
[ShuffleMonitor 14:02:02] 文件数: 184, 总大小: 2.96GB (峰值: 184文件, 2.96GB)
[ShuffleMonitor 14:02:03] 文件数: 188, 总大小: 3.11GB (峰值: 188文件, 3.11GB)
[ShuffleMonitor 14:02:04] 文件数: 192, 总大小: 3.25GB (峰值: 192文件, 3.25GB)
[ShuffleMonitor 14:02:05] 文件数: 197, 总大小: 3.40GB (峰值: 197文件, 3.40GB)
[ShuffleMonitor 14:02:06] 文件数: 197, 总大小: 3.41GB (峰值: 197文件, 3.41GB)
[ShuffleMonitor 14:02:07] 文件数: 199, 总大小: 3.54GB (峰值: 199文件, 3.54GB)
[ShuffleMonitor 14:02:08] 文件数: 203, 总大小: 3.69GB (峰值: 203文件, 3.69GB)
[ShuffleMonitor 14:02:09] 文件数: 209, 总大小: 3.84GB (峰值: 209文件, 3.84GB)
[ShuffleMonitor 14:02:10] 文件数: 213, 总大小: 4.01GB (峰值: 213文件, 4.01GB)
[ShuffleMonitor 14:02:11] 文件数: 217, 总大小: 4.18GB (峰值: 217文件, 4.18GB)
[ShuffleMonitor 14:02:12] 文件数: 223, 总大小: 4.35GB (峰值: 223文件, 4.35GB)
[ShuffleMonitor 14:02:13] 文件数: 227, 总大小: 4.53GB (峰值: 227文件, 4.53GB)
[ShuffleMonitor 14:02:14] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:15] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:16] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:17] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:18] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:19] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:20] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:21] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:22] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:23] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:24] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:25] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:26] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:27] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:28] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:29] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:30] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:31] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:32] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:33] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:34] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:35] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:36] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:37] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:38] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:39] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:40] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:41] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:42] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:43] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:44] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:45] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:46] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:47] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:48] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:49] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:50] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:51] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:52] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:53] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:54] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:55] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:56] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:57] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:58] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:02:59] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:00] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:01] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:02] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:03] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:04] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:05] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:06] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:08] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:09] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:10] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:11] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:12] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:13] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:14] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:15] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:16] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:17] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:18] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:19] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:20] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:21] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:22] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:23] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:24] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:25] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:26] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:27] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:28] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:29] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:30] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:31] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:32] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:33] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:34] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:35] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:36] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:37] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:38] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:39] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:40] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:41] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:42] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:43] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:44] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:45] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:46] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:47] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:48] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:49] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:50] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:51] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:52] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:53] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:54] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:55] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:56] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:57] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:58] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:03:59] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:00] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:01] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:02] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:03] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:04] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:05] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:06] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:07] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:08] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:09] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:10] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:11] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:12] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:13] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:14] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:15] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:16] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:17] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:18] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:19] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:20] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:21] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:22] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:23] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:24] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:25] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:26] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:27] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:28] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:29] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:30] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:31] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:32] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:33] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:34] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:35] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:36] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:37] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:38] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:39] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:40] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
[ShuffleMonitor 14:04:42] 文件数: 230, 总大小: 4.63GB (峰值: 230文件, 4.63GB)
25/12/04 14:06:53 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 124018 ms exceeds timeout 120000 ms
25/12/04 14:06:53 ERROR TaskSchedulerImpl: Lost executor 1 on 49.52.27.65: Executor heartbeat timed out after 124018 ms
25/12/04 14:06:53 WARN TaskSetManager: Lost task 435.0 in stage 3.0 (TID 636, 49.52.27.65): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 124018 ms
25/12/04 14:06:53 WARN TaskSetManager: Lost task 436.0 in stage 3.0 (TID 637, 49.52.27.65): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 124018 ms
25/12/04 14:07:04 ERROR TaskSchedulerImpl: Lost executor 1 on 49.52.27.65: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 14:09:53 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 153629 ms exceeds timeout 120000 ms
25/12/04 14:09:53 ERROR TaskSchedulerImpl: Lost executor 0 on 49.52.27.60: Executor heartbeat timed out after 153629 ms
25/12/04 14:09:53 WARN TaskSetManager: Lost task 436.1 in stage 3.0 (TID 701, 49.52.27.60): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 153629 ms
25/12/04 14:09:53 WARN TaskSetManager: Lost task 437.0 in stage 3.0 (TID 638, 49.52.27.60): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 153629 ms
25/12/04 14:10:03 ERROR TaskSchedulerImpl: Lost executor 0 on 49.52.27.60: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 14:10:14 WARN TaskSetManager: Lost task 437.1 in stage 3.0 (TID 703, 49.52.27.65): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=437, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:548)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:544)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:544)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:155)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

)
25/12/04 14:10:14 WARN TaskSetManager: Lost task 435.1 in stage 3.0 (TID 702, 49.52.27.65): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=435, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:548)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:544)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:544)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:155)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

)
25/12/04 14:10:14 WARN TaskSetManager: Lost task 436.2 in stage 3.0 (TID 704, 49.52.27.65): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=436, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:548)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:544)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:544)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:155)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

)
25/12/04 14:14:33 ERROR TransportChannelHandler: Connection to 49.52.27.65/49.52.27.65:54266 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.network.timeout if this is wrong.
25/12/04 14:14:33 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from 49.52.27.65/49.52.27.65:54266 is closed
25/12/04 14:14:33 ERROR TaskSchedulerImpl: Lost executor 2 on 49.52.27.65: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 14:14:33 WARN TaskSetManager: Lost task 0.0 in stage 3.1 (TID 805, 49.52.27.65): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 14:14:33 WARN TaskSetManager: Lost task 2.0 in stage 3.1 (TID 807, 49.52.27.65): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 14:14:33 ERROR ContextCleaner: Error cleaning broadcast 4
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
	at org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:144)
	at org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:228)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast(TorrentBroadcastFactory.scala:45)
	at org.apache.spark.broadcast.BroadcastManager.unbroadcast(BroadcastManager.scala:67)
	at org.apache.spark.ContextCleaner.doCleanupBroadcast(ContextCleaner.scala:233)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(ContextCleaner.scala:189)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(ContextCleaner.scala:180)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:180)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1181)
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:173)
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:68)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	... 12 more
25/12/04 14:14:33 WARN BlockManagerMaster: Failed to remove broadcast 4 with removeFromMaster = true - Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.util.Failure$$anonfun$recover$1.apply(Try.scala:185)
	at scala.util.Try$.apply(Try.scala:161)
	at scala.util.Failure.recover(Try.scala:185)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:324)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark-project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:133)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.processBatch$1(Future.scala:643)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply$mcV$sp(Future.scala:658)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch$$anonfun$run$1.apply(Future.scala:635)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.Future$InternalCallbackExecutor$Batch.run(Future.scala:634)
	at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:685)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:241)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply in 120 seconds
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:242)
	... 7 more
25/12/04 14:14:33 WARN NettyRpcEnv: Ignored failure: java.io.IOException: Connection from 49.52.27.65/49.52.27.65:54266 closed
25/12/04 14:16:28 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8747610051404206590, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=47]}} to 49.52.27.65/49.52.27.65:34190; closing connection
java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:470)
	at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:115)
	at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:100)
	at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254)
	at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237)
	at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644)
	at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644)
	at io.netty.channel.ChannelDuplexHandler.flush(ChannelDuplexHandler.java:117)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.access$1500(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:961)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:750)
25/12/04 14:16:28 WARN SparkDeploySchedulerBackend: Ignored task status update (805 state FAILED) from unknown executor with ID 2
25/12/04 14:16:28 WARN SparkDeploySchedulerBackend: Ignored task status update (807 state FAILED) from unknown executor with ID 2
