{"Event":"SparkListenerLogStart","Spark Version":"1.6.3"}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"driver","Host":"49.52.27.65","Port":40427},"Maximum Memory":150208512,"Timestamp":1764843027529}
{"Event":"SparkListenerEnvironmentUpdate","JVM Information":{"Java Home":"/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre","Java Version":"1.8.0_472-472 (OpenLogic-OpenJDK)","Scala Version":"version 2.10.5"},"Spark Properties":{"spark.serializer":"org.apache.spark.serializer.KryoSerializer","spark.driver.host":"49.52.27.65","spark.eventLog.enabled":"true","spark.shuffle.manager":"sort","spark.sql.adaptive.enabled":"false","spark.shuffle.sort.bypassMergeThreshold":"1","spark.driver.port":"42987","spark.shuffle.service.enabled":"false","spark.jars":"file:/home/djk/Documents/shuffle-experiment/target/scala-2.10/spark-shuffle-experiment-1.0.0.jar","spark.app.name":"Shuffle-Exp-sort-uniform","spark.cores.max":"4","spark.scheduler.mode":"FIFO","spark.driver.memory":"512M","spark.executor.instances":"2","spark.executor.id":"driver","spark.submit.deployMode":"client","spark.master":"spark://49.52.27.113:7077","spark.executor.memory":"1G","spark.local.dir":"/tmp/spark/work","spark.eventLog.dir":"logs/shuffle_file_test_20251204_100947/events_sort_uniform","spark.dynamicAllocation.enabled":"false","spark.executor.cores":"2","spark.externalBlockStore.folderName":"spark-769b4265-be2e-448b-87a8-82d77c2145eb","spark.shuffle.consolidateFiles":"false","spark.app.id":"app-20251204051017-0116"},"System Properties":{"java.io.tmpdir":"/tmp","line.separator":"\n","path.separator":":","sun.management.compiler":"HotSpot 64-Bit Tiered Compilers","SPARK_SUBMIT":"true","sun.cpu.endian":"little","java.specification.maintenance.version":"6","java.specification.version":"1.8","java.vm.specification.name":"Java Virtual Machine Specification","java.vendor":"OpenLogic-OpenJDK","java.vm.specification.version":"1.8","user.home":"/home/djk","file.encoding.pkg":"sun.io","sun.nio.ch.bugLevel":"","sun.arch.data.model":"64","sun.boot.library.path":"/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre/lib/amd64","user.dir":"/home/djk/Documents/shuffle-experiment","java.library.path":"/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib","sun.cpu.isalist":"","os.arch":"amd64","java.vm.version":"25.472-b08","java.endorsed.dirs":"/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre/lib/endorsed","java.runtime.version":"1.8.0_472-472-b08","java.vm.info":"mixed mode","java.ext.dirs":"/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre/lib/ext:/usr/java/packages/lib/ext","java.runtime.name":"OpenJDK Runtime Environment","file.separator":"/","java.class.version":"52.0","java.specification.name":"Java Platform API Specification","sun.boot.class.path":"/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre/lib/resources.jar:/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre/lib/rt.jar:/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre/lib/sunrsasign.jar:/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre/lib/jsse.jar:/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre/lib/jce.jar:/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre/lib/charsets.jar:/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre/lib/jfr.jar:/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre/classes","file.encoding":"UTF-8","user.timezone":"Etc/UTC","java.specification.vendor":"Oracle Corporation","sun.java.launcher":"SUN_STANDARD","os.version":"6.8.0-87-generic","sun.os.patch.level":"unknown","java.vm.specification.vendor":"Oracle Corporation","user.country":"US","sun.jnu.encoding":"UTF-8","user.language":"en","java.vendor.url":"http://java.oracle.com/","java.awt.printerjob":"sun.print.PSPrinterJob","java.awt.graphicsenv":"sun.awt.X11GraphicsEnvironment","awt.toolkit":"sun.awt.X11.XToolkit","os.name":"Linux","java.vm.vendor":"OpenLogic-OpenJDK","java.vendor.url.bug":"http://bugreport.sun.com/bugreport/","user.name":"djk","java.vm.name":"OpenJDK 64-Bit Server VM","sun.java.command":"org.apache.spark.deploy.SparkSubmit --master spark://49.52.27.113:7077 --deploy-mode client --conf spark.driver.memory=512M --conf spark.shuffle.sort.bypassMergeThreshold=1 --conf spark.sql.adaptive.enabled=false --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.eventLog.enabled=true --conf spark.eventLog.dir=logs/shuffle_file_test_20251204_100947/events_sort_uniform --conf spark.shuffle.service.enabled=false --conf spark.shuffle.manager=sort --conf spark.executor.instances=2 --conf spark.dynamicAllocation.enabled=false --conf spark.shuffle.consolidateFiles=false --class edu.ecnu.MainEntry --executor-memory 1G --executor-cores 2 --total-executor-cores 4 target/scala-2.10/spark-shuffle-experiment-1.0.0.jar tpch sort file:///home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl uniform medium","java.home":"/home/djk/java/openlogic-openjdk-8u472-b08-linux-x64/jre","java.version":"1.8.0_472-472","sun.io.unicode.encoding":"UnicodeLittle"},"Classpath Entries":{"http://49.52.27.65:35289/jars/spark-shuffle-experiment-1.0.0.jar":"Added By User","/tmp/spark/lib/datanucleus-core-3.2.10.jar":"System Classpath","/tmp/spark/lib/spark-assembly-1.6.3-hadoop2.6.0.jar":"System Classpath","/tmp/spark/conf/":"System Classpath","/tmp/spark/lib/datanucleus-rdbms-3.2.9.jar":"System Classpath","/tmp/spark/lib/datanucleus-api-jdo-3.2.6.jar":"System Classpath"}}
{"Event":"SparkListenerApplicationStart","App Name":"Shuffle-Exp-sort-uniform","App ID":"app-20251204051017-0116","Timestamp":1764843015779,"User":"djk"}
{"Event":"SparkListenerJobStart","Job ID":0,"Submission Time":1764843034554,"Stage Infos":[{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"repartition at DataGenerator.scala:145","Number of Tasks":116,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"repartition\"}","Callsite":"repartition at DataGenerator.scala:145","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":116,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"file:///home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DataGenerator.scala:144","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":116,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"file:///home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DataGenerator.scala:144","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":116,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.repartition(RDD.scala:371)\nedu.ecnu.DataGenerator$.loadTpch(DataGenerator.scala:145)\nedu.ecnu.TPCHTest$.run(TPCHTest.scala:53)\nedu.ecnu.MainEntry$.main(MainEntry.scala:29)\nedu.ecnu.MainEntry.main(MainEntry.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},{"Stage ID":1,"Stage Attempt ID":0,"Stage Name":"count at TPCHTest.scala:58","Number of Tasks":100,"RDD Info":[{"RDD ID":12,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"8\",\"name\":\"TungstenExchange\"}","Callsite":"count at TPCHTest.scala:58","Parent IDs":[11],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":100,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":4,"Name":"CoalescedRDD","Scope":"{\"id\":\"1\",\"name\":\"repartition\"}","Callsite":"repartition at DataGenerator.scala:145","Parent IDs":[3],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":100,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":8,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"4\",\"name\":\"Project\"}","Callsite":"cache at TPCHTest.scala:57","Parent IDs":[7],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":100,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":11,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"9\",\"name\":\"TungstenAggregate\"}","Callsite":"count at TPCHTest.scala:58","Parent IDs":[10],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":100,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":10,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"10\",\"name\":\"InMemoryColumnarTableScan\"}","Callsite":"count at TPCHTest.scala:58","Parent IDs":[9],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":100,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":3,"Name":"ShuffledRDD","Scope":"{\"id\":\"1\",\"name\":\"repartition\"}","Callsite":"repartition at DataGenerator.scala:145","Parent IDs":[2],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":100,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":7,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"3\",\"name\":\"mapPartitions\"}","Callsite":"createDataFrame at DataGenerator.scala:178","Parent IDs":[6],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":100,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":5,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"repartition\"}","Callsite":"repartition at DataGenerator.scala:145","Parent IDs":[4],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":100,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":6,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"2\",\"name\":\"map\"}","Callsite":"map at DataGenerator.scala:147","Parent IDs":[5],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":100,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":9,"Name":"Project [_1#0 AS key#4,_2#1 AS pad1#5,_3#2 AS pad2#6,_4#3 AS payload#7]\n+- Scan ExistingRDD[_1#0,_2#1,_3#2,_4#3] \n","Scope":"{\"id\":\"6\",\"name\":\"mapPartitionsInternal\"}","Callsite":"cache at TPCHTest.scala:57","Parent IDs":[8],"Storage Level":{"Use Disk":true,"Use Memory":true,"Use ExternalBlockStore":false,"Deserialized":true,"Replication":1},"Number of Partitions":100,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[0],"Details":"org.apache.spark.sql.DataFrame.count(DataFrame.scala:1515)\nedu.ecnu.TPCHTest$.run(TPCHTest.scala:58)\nedu.ecnu.MainEntry$.main(MainEntry.scala:29)\nedu.ecnu.MainEntry.main(MainEntry.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},{"Stage ID":2,"Stage Attempt ID":0,"Stage Name":"count at TPCHTest.scala:58","Number of Tasks":1,"RDD Info":[{"RDD ID":15,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"11\",\"name\":\"map\"}","Callsite":"count at TPCHTest.scala:58","Parent IDs":[14],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":13,"Name":"ShuffledRowRDD","Scope":"{\"id\":\"8\",\"name\":\"TungstenExchange\"}","Callsite":"count at TPCHTest.scala:58","Parent IDs":[12],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":14,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"7\",\"name\":\"TungstenAggregate\"}","Callsite":"count at TPCHTest.scala:58","Parent IDs":[13],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":1,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[1],"Details":"org.apache.spark.sql.DataFrame.count(DataFrame.scala:1515)\nedu.ecnu.TPCHTest$.run(TPCHTest.scala:58)\nedu.ecnu.MainEntry$.main(MainEntry.scala:29)\nedu.ecnu.MainEntry.main(MainEntry.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]}],"Stage IDs":[0,1,2],"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"12\",\"name\":\"collect\"}","spark.sql.execution.id":"0"}}
{"Event":"SparkListenerStageSubmitted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"repartition at DataGenerator.scala:145","Number of Tasks":116,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"repartition\"}","Callsite":"repartition at DataGenerator.scala:145","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":116,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"file:///home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DataGenerator.scala:144","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":116,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"file:///home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DataGenerator.scala:144","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":116,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.repartition(RDD.scala:371)\nedu.ecnu.DataGenerator$.loadTpch(DataGenerator.scala:145)\nedu.ecnu.TPCHTest$.run(TPCHTest.scala:53)\nedu.ecnu.MainEntry$.main(MainEntry.scala:29)\nedu.ecnu.MainEntry.main(MainEntry.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Accumulables":[]},"Properties":{"spark.rdd.scope.noOverride":"true","spark.rdd.scope":"{\"id\":\"12\",\"name\":\"collect\"}","spark.sql.execution.id":"0"}}
{"Event":"SparkListenerExecutorAdded","Timestamp":1764843038642,"Executor ID":"0","Executor Info":{"Host":"49.52.27.60","Total Cores":2,"Log Urls":{"stdout":"http://49.52.27.60:8082/logPage/?appId=app-20251204051017-0116&executorId=0&logType=stdout","stderr":"http://49.52.27.60:8082/logPage/?appId=app-20251204051017-0116&executorId=0&logType=stderr"}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1764843038644,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":1,"Index":1,"Attempt":0,"Launch Time":1764843038659,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerBlockManagerAdded","Block Manager ID":{"Executor ID":"0","Host":"49.52.27.60","Port":34993},"Maximum Memory":535953408,"Timestamp":1764843038708}
{"Event":"SparkListenerExecutorAdded","Timestamp":1764843038716,"Executor ID":"1","Executor Info":{"Host":"49.52.27.65","Total Cores":2,"Log Urls":{"stdout":"http://49.52.27.65:8081/logPage/?appId=app-20251204051017-0116&executorId=1&logType=stdout","stderr":"http://49.52.27.65:8081/logPage/?appId=app-20251204051017-0116&executorId=1&logType=stderr"}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":2,"Index":2,"Attempt":0,"Launch Time":1764843038717,"Executor ID":"1","Host":"49.52.27.65","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":3,"Index":3,"Attempt":0,"Launch Time":1764843038717,"Executor ID":"1","Host":"49.52.27.65","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":4,"Index":4,"Attempt":0,"Launch Time":1764843040829,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":5,"Index":5,"Attempt":0,"Launch Time":1764843040830,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":745,"Result Size":0,"JVM GC Time":158,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":1,"Index":1,"Attempt":0,"Launch Time":1764843038659,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040852,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":745,"Result Size":0,"JVM GC Time":158,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":745,"Result Size":0,"JVM GC Time":158,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":0,"Index":0,"Attempt":0,"Launch Time":1764843038644,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040855,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":745,"Result Size":0,"JVM GC Time":158,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":6,"Index":0,"Attempt":1,"Launch Time":1764843040856,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040894,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":12,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":5,"Index":5,"Attempt":0,"Launch Time":1764843040830,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040856,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":12,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":7,"Index":5,"Attempt":1,"Launch Time":1764843040857,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040894,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":15,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":4,"Index":4,"Attempt":0,"Launch Time":1764843040829,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040858,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":15,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":8,"Index":4,"Attempt":1,"Launch Time":1764843040892,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040911,"Failed":true,"Accumulables":[]}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":9,"Index":1,"Attempt":1,"Launch Time":1764843040893,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":7,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":7,"Index":5,"Attempt":1,"Launch Time":1764843040857,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040894,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":7,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":9,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":6,"Index":0,"Attempt":1,"Launch Time":1764843040856,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040894,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":9,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":10,"Index":0,"Attempt":2,"Launch Time":1764843040911,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":11,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":8,"Index":4,"Attempt":1,"Launch Time":1764843040892,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040911,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":11,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":11,"Index":4,"Attempt":2,"Launch Time":1764843040915,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":14,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":9,"Index":1,"Attempt":1,"Launch Time":1764843040893,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040916,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":14,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":12,"Index":1,"Attempt":2,"Launch Time":1764843040922,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":5,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":10,"Index":0,"Attempt":2,"Launch Time":1764843040911,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040922,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":5,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":13,"Index":0,"Attempt":3,"Launch Time":1764843040930,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":8,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":11,"Index":4,"Attempt":2,"Launch Time":1764843040915,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040931,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":8,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":14,"Index":4,"Attempt":3,"Launch Time":1764843040933,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":6,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":12,"Index":1,"Attempt":2,"Launch Time":1764843040922,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040934,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":6,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":0,"Task Info":{"Task ID":15,"Index":1,"Attempt":3,"Launch Time":1764843040939,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":0,"Failed":false,"Accumulables":[]}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":4,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":13,"Index":0,"Attempt":3,"Launch Time":1764843040930,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040940,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":4,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerStageCompleted","Stage Info":{"Stage ID":0,"Stage Attempt ID":0,"Stage Name":"repartition at DataGenerator.scala:145","Number of Tasks":116,"RDD Info":[{"RDD ID":2,"Name":"MapPartitionsRDD","Scope":"{\"id\":\"1\",\"name\":\"repartition\"}","Callsite":"repartition at DataGenerator.scala:145","Parent IDs":[1],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":116,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":1,"Name":"file:///home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DataGenerator.scala:144","Parent IDs":[0],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":116,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0},{"RDD ID":0,"Name":"file:///home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl","Scope":"{\"id\":\"0\",\"name\":\"textFile\"}","Callsite":"textFile at DataGenerator.scala:144","Parent IDs":[],"Storage Level":{"Use Disk":false,"Use Memory":false,"Use ExternalBlockStore":false,"Deserialized":false,"Replication":1},"Number of Partitions":116,"Number of Cached Partitions":0,"Memory Size":0,"ExternalBlockStore Size":0,"Disk Size":0}],"Parent IDs":[],"Details":"org.apache.spark.rdd.RDD.repartition(RDD.scala:371)\nedu.ecnu.DataGenerator$.loadTpch(DataGenerator.scala:145)\nedu.ecnu.TPCHTest$.run(TPCHTest.scala:53)\nedu.ecnu.MainEntry$.main(MainEntry.scala:29)\nedu.ecnu.MainEntry.main(MainEntry.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)\norg.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)\norg.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)","Submission Time":1764843034602,"Completion Time":1764843040948,"Failure Reason":"Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 13, 49.52.27.60): java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:","Accumulables":[]}}
{"Event":"SparkListenerJobEnd","Job ID":0,"Completion Time":1764843040951,"Job Result":{"Result":"JobFailed","Exception":{"Message":"Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 13, 49.52.27.60): java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:","Stack Trace":[{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages","File Name":"DAGScheduler.scala","Line Number":1431},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":1419},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":1418},{"Declaring Class":"scala.collection.mutable.ResizableArray$class","Method Name":"foreach","File Name":"ResizableArray.scala","Line Number":59},{"Declaring Class":"scala.collection.mutable.ArrayBuffer","Method Name":"foreach","File Name":"ArrayBuffer.scala","Line Number":47},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"abortStage","File Name":"DAGScheduler.scala","Line Number":1418},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":799},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1","Method Name":"apply","File Name":"DAGScheduler.scala","Line Number":799},{"Declaring Class":"scala.Option","Method Name":"foreach","File Name":"Option.scala","Line Number":236},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"handleTaskSetFailed","File Name":"DAGScheduler.scala","Line Number":799},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"doOnReceive","File Name":"DAGScheduler.scala","Line Number":1640},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":1599},{"Declaring Class":"org.apache.spark.scheduler.DAGSchedulerEventProcessLoop","Method Name":"onReceive","File Name":"DAGScheduler.scala","Line Number":1588},{"Declaring Class":"org.apache.spark.util.EventLoop$$anon$1","Method Name":"run","File Name":"EventLoop.scala","Line Number":48},{"Declaring Class":"org.apache.spark.scheduler.DAGScheduler","Method Name":"runJob","File Name":"DAGScheduler.scala","Line Number":620},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1832},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1845},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1858},{"Declaring Class":"org.apache.spark.SparkContext","Method Name":"runJob","File Name":"SparkContext.scala","Line Number":1929},{"Declaring Class":"org.apache.spark.rdd.RDD$$anonfun$collect$1","Method Name":"apply","File Name":"RDD.scala","Line Number":927},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":150},{"Declaring Class":"org.apache.spark.rdd.RDDOperationScope$","Method Name":"withScope","File Name":"RDDOperationScope.scala","Line Number":111},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"withScope","File Name":"RDD.scala","Line Number":316},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"collect","File Name":"RDD.scala","Line Number":926},{"Declaring Class":"org.apache.spark.sql.execution.SparkPlan","Method Name":"executeCollect","File Name":"SparkPlan.scala","Line Number":166},{"Declaring Class":"org.apache.spark.sql.execution.SparkPlan","Method Name":"executeCollectPublic","File Name":"SparkPlan.scala","Line Number":174},{"Declaring Class":"org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1","Method Name":"apply","File Name":"DataFrame.scala","Line Number":1500},{"Declaring Class":"org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1","Method Name":"apply","File Name":"DataFrame.scala","Line Number":1500},{"Declaring Class":"org.apache.spark.sql.execution.SQLExecution$","Method Name":"withNewExecutionId","File Name":"SQLExecution.scala","Line Number":56},{"Declaring Class":"org.apache.spark.sql.DataFrame","Method Name":"withNewExecutionId","File Name":"DataFrame.scala","Line Number":2087},{"Declaring Class":"org.apache.spark.sql.DataFrame","Method Name":"org$apache$spark$sql$DataFrame$$execute$1","File Name":"DataFrame.scala","Line Number":1499},{"Declaring Class":"org.apache.spark.sql.DataFrame","Method Name":"org$apache$spark$sql$DataFrame$$collect","File Name":"DataFrame.scala","Line Number":1506},{"Declaring Class":"org.apache.spark.sql.DataFrame$$anonfun$count$1","Method Name":"apply","File Name":"DataFrame.scala","Line Number":1516},{"Declaring Class":"org.apache.spark.sql.DataFrame$$anonfun$count$1","Method Name":"apply","File Name":"DataFrame.scala","Line Number":1515},{"Declaring Class":"org.apache.spark.sql.DataFrame","Method Name":"withCallback","File Name":"DataFrame.scala","Line Number":2100},{"Declaring Class":"org.apache.spark.sql.DataFrame","Method Name":"count","File Name":"DataFrame.scala","Line Number":1515},{"Declaring Class":"edu.ecnu.TPCHTest$","Method Name":"run","File Name":"TPCHTest.scala","Line Number":58},{"Declaring Class":"edu.ecnu.MainEntry$","Method Name":"main","File Name":"MainEntry.scala","Line Number":29},{"Declaring Class":"edu.ecnu.MainEntry","Method Name":"main","File Name":"MainEntry.scala","Line Number":-1},{"Declaring Class":"sun.reflect.NativeMethodAccessorImpl","Method Name":"invoke0","File Name":"NativeMethodAccessorImpl.java","Line Number":-2},{"Declaring Class":"sun.reflect.NativeMethodAccessorImpl","Method Name":"invoke","File Name":"NativeMethodAccessorImpl.java","Line Number":62},{"Declaring Class":"sun.reflect.DelegatingMethodAccessorImpl","Method Name":"invoke","File Name":"DelegatingMethodAccessorImpl.java","Line Number":43},{"Declaring Class":"java.lang.reflect.Method","Method Name":"invoke","File Name":"Method.java","Line Number":498},{"Declaring Class":"org.apache.spark.deploy.SparkSubmit$","Method Name":"org$apache$spark$deploy$SparkSubmit$$runMain","File Name":"SparkSubmit.scala","Line Number":731},{"Declaring Class":"org.apache.spark.deploy.SparkSubmit$","Method Name":"doRunMain$1","File Name":"SparkSubmit.scala","Line Number":181},{"Declaring Class":"org.apache.spark.deploy.SparkSubmit$","Method Name":"submit","File Name":"SparkSubmit.scala","Line Number":206},{"Declaring Class":"org.apache.spark.deploy.SparkSubmit$","Method Name":"main","File Name":"SparkSubmit.scala","Line Number":121},{"Declaring Class":"org.apache.spark.deploy.SparkSubmit","Method Name":"main","File Name":"SparkSubmit.scala","Line Number":-1}]}}}
{"Event":"SparkListenerApplicationEnd","Timestamp":1764843040953}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":7,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":15,"Index":1,"Attempt":3,"Launch Time":1764843040939,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040956,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":7,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":0,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"ExceptionFailure","Class Name":"java.io.FileNotFoundException","Description":"File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist","Stack Trace":[{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"deprecatedGetFileStatus","File Name":"RawLocalFileSystem.java","Line Number":534},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileLinkStatusInternal","File Name":"RawLocalFileSystem.java","Line Number":747},{"Declaring Class":"org.apache.hadoop.fs.RawLocalFileSystem","Method Name":"getFileStatus","File Name":"RawLocalFileSystem.java","Line Number":524},{"Declaring Class":"org.apache.hadoop.fs.FilterFileSystem","Method Name":"getFileStatus","File Name":"FilterFileSystem.java","Line Number":409},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker","Method Name":"<init>","File Name":"ChecksumFileSystem.java","Line Number":140},{"Declaring Class":"org.apache.hadoop.fs.ChecksumFileSystem","Method Name":"open","File Name":"ChecksumFileSystem.java","Line Number":341},{"Declaring Class":"org.apache.hadoop.fs.FileSystem","Method Name":"open","File Name":"FileSystem.java","Line Number":766},{"Declaring Class":"org.apache.hadoop.mapred.LineRecordReader","Method Name":"<init>","File Name":"LineRecordReader.java","Line Number":108},{"Declaring Class":"org.apache.hadoop.mapred.TextInputFormat","Method Name":"getRecordReader","File Name":"TextInputFormat.java","Line Number":67},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD$$anon$1","Method Name":"<init>","File Name":"HadoopRDD.scala","Line Number":240},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":211},{"Declaring Class":"org.apache.spark.rdd.HadoopRDD","Method Name":"compute","File Name":"HadoopRDD.scala","Line Number":101},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.rdd.MapPartitionsRDD","Method Name":"compute","File Name":"MapPartitionsRDD.scala","Line Number":38},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"computeOrReadCheckpoint","File Name":"RDD.scala","Line Number":306},{"Declaring Class":"org.apache.spark.rdd.RDD","Method Name":"iterator","File Name":"RDD.scala","Line Number":270},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":73},{"Declaring Class":"org.apache.spark.scheduler.ShuffleMapTask","Method Name":"runTask","File Name":"ShuffleMapTask.scala","Line Number":41},{"Declaring Class":"org.apache.spark.scheduler.Task","Method Name":"run","File Name":"Task.scala","Line Number":89},{"Declaring Class":"org.apache.spark.executor.Executor$TaskRunner","Method Name":"run","File Name":"Executor.scala","Line Number":227},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor","Method Name":"runWorker","File Name":"ThreadPoolExecutor.java","Line Number":1149},{"Declaring Class":"java.util.concurrent.ThreadPoolExecutor$Worker","Method Name":"run","File Name":"ThreadPoolExecutor.java","Line Number":624},{"Declaring Class":"java.lang.Thread","Method Name":"run","File Name":"Thread.java","Line Number":750}],"Full Stack Trace":"java.io.FileNotFoundException: File file:/home/djk/Downloads/tpch-dbgen/gen_data/lineitem-medium.tbl does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:534)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:766)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:108)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:240)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:211)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n","Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":13,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}},"Task Info":{"Task ID":14,"Index":4,"Attempt":3,"Launch Time":1764843040933,"Executor ID":"0","Host":"49.52.27.60","Locality":"PROCESS_LOCAL","Speculative":false,"Getting Result Time":0,"Finish Time":1764843040957,"Failed":true,"Accumulables":[]},"Task Metrics":{"Host Name":"49.52.27.60","Executor Deserialize Time":0,"Executor Run Time":13,"Result Size":0,"JVM GC Time":0,"Result Serialization Time":0,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":0,"Shuffle Write Time":0,"Shuffle Records Written":0},"Input Metrics":{"Data Read Method":"Hadoop","Bytes Read":0,"Records Read":0}}}
