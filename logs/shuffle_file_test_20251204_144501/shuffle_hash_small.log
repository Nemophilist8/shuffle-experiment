=== Dispatcher: Routing to task [file] ===
Running File Count Monitor with args: hash small
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/12/04 14:45:04 INFO SparkContext: Running Spark version 1.6.3
25/12/04 14:45:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/12/04 14:45:04 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
25/12/04 14:45:04 WARN Utils: Your hostname, hygon7490 resolves to a loopback address: 127.0.0.1; using 49.52.27.65 instead (on interface ens2f1np1)
25/12/04 14:45:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/12/04 14:45:04 INFO SecurityManager: Changing view acls to: djk
25/12/04 14:45:04 INFO SecurityManager: Changing modify acls to: djk
25/12/04 14:45:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(djk); users with modify permissions: Set(djk)
25/12/04 14:45:04 INFO Utils: Successfully started service 'sparkDriver' on port 37593.
25/12/04 14:45:05 INFO Slf4jLogger: Slf4jLogger started
25/12/04 14:45:05 INFO Remoting: Starting remoting
25/12/04 14:45:05 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@49.52.27.65:37973]
25/12/04 14:45:05 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 37973.
25/12/04 14:45:05 INFO SparkEnv: Registering MapOutputTracker
25/12/04 14:45:05 INFO SparkEnv: Registering BlockManagerMaster
25/12/04 14:45:05 INFO DiskBlockManager: Created local directory at /tmp/spark/work/blockmgr-75f6b2c2-78e1-4514-85da-38757ef03a97
25/12/04 14:45:05 INFO MemoryStore: MemoryStore started with capacity 143.3 MB
25/12/04 14:45:05 INFO SparkEnv: Registering OutputCommitCoordinator
25/12/04 14:45:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/12/04 14:45:05 INFO SparkUI: Started SparkUI at http://49.52.27.65:4040
25/12/04 14:45:05 INFO HttpFileServer: HTTP File server directory is /tmp/spark/work/spark-f182dfde-7b47-4ea1-907d-bfcb9ab4008d/httpd-9963b970-5a61-469a-83f5-594bc9083fe9
25/12/04 14:45:05 INFO HttpServer: Starting HTTP Server
25/12/04 14:45:05 INFO Utils: Successfully started service 'HTTP file server' on port 38785.
25/12/04 14:45:05 INFO SparkContext: Added JAR file:/home/djk/Documents/shuffle-experiment/target/scala-2.10/spark-shuffle-experiment-1.0.0.jar at http://49.52.27.65:38785/jars/spark-shuffle-experiment-1.0.0.jar with timestamp 1764859505686
25/12/04 14:45:05 INFO AppClient$ClientEndpoint: Connecting to master spark://49.52.27.113:7077...
25/12/04 14:45:15 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20251204094505-0170
25/12/04 14:45:15 INFO AppClient$ClientEndpoint: Executor added: app-20251204094505-0170/0 on worker-20251203224515-49.52.27.60-35123 (49.52.27.60:35123) with 2 cores
25/12/04 14:45:15 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204094505-0170/0 on hostPort 49.52.27.60:35123 with 2 cores, 8.0 GB RAM
25/12/04 14:45:15 INFO AppClient$ClientEndpoint: Executor added: app-20251204094505-0170/1 on worker-20251203144508-49.52.27.65-34725 (49.52.27.65:34725) with 2 cores
25/12/04 14:45:15 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204094505-0170/1 on hostPort 49.52.27.65:34725 with 2 cores, 8.0 GB RAM
25/12/04 14:45:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34503.
25/12/04 14:45:15 INFO NettyBlockTransferService: Server created on 34503
25/12/04 14:45:15 INFO BlockManagerMaster: Trying to register BlockManager
25/12/04 14:45:15 INFO BlockManagerMasterEndpoint: Registering block manager 49.52.27.65:34503 with 143.3 MB RAM, BlockManagerId(driver, 49.52.27.65, 34503)
25/12/04 14:45:15 INFO BlockManagerMaster: Registered BlockManager
25/12/04 14:45:15 INFO AppClient$ClientEndpoint: Executor updated: app-20251204094505-0170/1 is now RUNNING
25/12/04 14:45:15 INFO AppClient$ClientEndpoint: Executor updated: app-20251204094505-0170/0 is now RUNNING
25/12/04 14:45:26 INFO EventLoggingListener: Logging events to file:/tmp/spark-events/app-20251204094505-0170
25/12/04 14:45:26 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
=== Shuffle File Monitor 实验 ===
3. Spark版本: 1.6.3
默认shuffle分区数: 200
请求的Shuffle Manager: hash
当前 Shuffle Manager: hash
Spark 本地目录: /tmp/spark/work
监控将每秒检查一次 /tmp/spark/work 目录中的 shuffle_* 文件
测试数据集: small
分区数: 20
>>> 正在进行数据预热 (Cache & Count)...
>>> 预热完成! 数据已驻留内存。耗时: 13491ms, 记录数: 500000
>>> 现在开始正式 Shuffle 实验 (此时文件应该会立即产生)

=== 开始运行 hash 实验 ===
启动Shuffle文件监控，每秒检查一次...
[ShuffleMonitor 14:45:40] 文件数: 2, 总大小: 0KB (峰值: 2文件, 0KB)
[ShuffleMonitor 14:45:41] 文件数: 819, 总大小: 48.44MB (峰值: 819文件, 48.44MB)
[ShuffleMonitor 14:45:42] 文件数: 1700, 总大小: 120.69MB (峰值: 1700文件, 120.69MB)
[ShuffleMonitor 警告] 检测到大量临时文件(1700 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:43] 文件数: 2722, 总大小: 220.31MB (峰值: 2722文件, 220.31MB)
[ShuffleMonitor 警告] 检测到大量临时文件(2722 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:44] 文件数: 3814, 总大小: 299.25MB (峰值: 3814文件, 299.25MB)
[ShuffleMonitor 警告] 检测到大量临时文件(3814 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:45] 文件数: 5010, 总大小: 383.71MB (峰值: 5010文件, 383.71MB)
[ShuffleMonitor 警告] 检测到大量临时文件(5010 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:46] 文件数: 5490, 总大小: 444.07MB (峰值: 5490文件, 444.07MB)
[ShuffleMonitor 警告] 检测到大量临时文件(5490 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:48] 文件数: 5490, 总大小: 444.07MB (峰值: 5490文件, 444.07MB)
[ShuffleMonitor 警告] 检测到大量临时文件(5490 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:49] 文件数: 5490, 总大小: 444.07MB (峰值: 5490文件, 444.07MB)
[ShuffleMonitor 警告] 检测到大量临时文件(5490 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:50] 文件数: 5490, 总大小: 444.07MB (峰值: 5490文件, 444.07MB)
[ShuffleMonitor 警告] 检测到大量临时文件(5490 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:51] 文件数: 5490, 总大小: 444.07MB (峰值: 5490文件, 444.07MB)
[ShuffleMonitor 警告] 检测到大量临时文件(5490 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:52] 文件数: 5490, 总大小: 444.07MB (峰值: 5490文件, 444.07MB)
[ShuffleMonitor 警告] 检测到大量临时文件(5490 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:53] 文件数: 5490, 总大小: 444.07MB (峰值: 5490文件, 444.07MB)
[ShuffleMonitor 警告] 检测到大量临时文件(5490 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:54] 文件数: 5490, 总大小: 444.07MB (峰值: 5490文件, 444.07MB)
[ShuffleMonitor 警告] 检测到大量临时文件(5490 个)，这可能是Hash Shuffle的特征
[ShuffleMonitor 14:45:55] 文件数: 5490, 总大小: 444.07MB (峰值: 5490文件, 444.07MB)
[ShuffleMonitor 警告] 检测到大量临时文件(5490 个)，这可能是Hash Shuffle的特征

[ShuffleMonitor 最终报告]
峰值文件数: 5490
峰值文件大小: 444.07MB
推测Shuffle类型: Hash Shuffle（文件数>500）

==================================================
测试结果 [hash]:
  执行耗时: 10415ms
  Shuffle数据量: 740.41 MB
  峰值Shuffle文件数: 5490 个
  峰值Shuffle文件大小: 454729KB (444.07MB)
  Shuffle类型分析: Hash Shuffle (检测到大量小文件: 5490 个)
  平均每个文件大小: 82KB
  ⚠️  警告: 检测到大量小文件(5490)个，建议使用Sort Shuffle
