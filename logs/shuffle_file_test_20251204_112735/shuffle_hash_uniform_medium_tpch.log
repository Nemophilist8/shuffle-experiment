=== Dispatcher: Routing to task [tpch] ===
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/12/04 11:27:35 INFO SparkContext: Running Spark version 1.6.3
25/12/04 11:27:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/12/04 11:27:36 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
25/12/04 11:27:36 WARN Utils: Your hostname, hygon7490 resolves to a loopback address: 127.0.0.1; using 49.52.27.65 instead (on interface ens2f1np1)
25/12/04 11:27:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/12/04 11:27:36 INFO SecurityManager: Changing view acls to: djk
25/12/04 11:27:36 INFO SecurityManager: Changing modify acls to: djk
25/12/04 11:27:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(djk); users with modify permissions: Set(djk)
25/12/04 11:27:36 INFO Utils: Successfully started service 'sparkDriver' on port 45225.
25/12/04 11:27:36 INFO Slf4jLogger: Slf4jLogger started
25/12/04 11:27:36 INFO Remoting: Starting remoting
25/12/04 11:27:36 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@49.52.27.65:44063]
25/12/04 11:27:36 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 44063.
25/12/04 11:27:36 INFO SparkEnv: Registering MapOutputTracker
25/12/04 11:27:36 INFO SparkEnv: Registering BlockManagerMaster
25/12/04 11:27:36 INFO DiskBlockManager: Created local directory at /tmp/spark/work/blockmgr-ef663e6b-7c17-41b7-ad8e-15dc1030f690
25/12/04 11:27:36 INFO MemoryStore: MemoryStore started with capacity 143.3 MB
25/12/04 11:27:36 INFO SparkEnv: Registering OutputCommitCoordinator
25/12/04 11:27:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/12/04 11:27:37 INFO SparkUI: Started SparkUI at http://49.52.27.65:4040
25/12/04 11:27:37 INFO HttpFileServer: HTTP File server directory is /tmp/spark/work/spark-8558f705-76c2-4d42-ba0e-57ecbd8dd824/httpd-08334332-046f-4a4f-bd98-ab390a40738d
25/12/04 11:27:37 INFO HttpServer: Starting HTTP Server
25/12/04 11:27:37 INFO Utils: Successfully started service 'HTTP file server' on port 41869.
25/12/04 11:27:37 INFO SparkContext: Added JAR file:/home/djk/Documents/shuffle-experiment/target/scala-2.10/spark-shuffle-experiment-1.0.0.jar at http://49.52.27.65:41869/jars/spark-shuffle-experiment-1.0.0.jar with timestamp 1764847657425
25/12/04 11:27:37 INFO AppClient$ClientEndpoint: Connecting to master spark://49.52.27.113:7077...
25/12/04 11:27:47 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20251204062737-0127
25/12/04 11:27:47 INFO AppClient$ClientEndpoint: Executor added: app-20251204062737-0127/0 on worker-20251203224515-49.52.27.60-35123 (49.52.27.60:35123) with 2 cores
25/12/04 11:27:47 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204062737-0127/0 on hostPort 49.52.27.60:35123 with 2 cores, 1024.0 MB RAM
25/12/04 11:27:47 INFO AppClient$ClientEndpoint: Executor added: app-20251204062737-0127/1 on worker-20251203144508-49.52.27.65-34725 (49.52.27.65:34725) with 2 cores
25/12/04 11:27:47 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204062737-0127/1 on hostPort 49.52.27.65:34725 with 2 cores, 1024.0 MB RAM
25/12/04 11:27:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36379.
25/12/04 11:27:47 INFO NettyBlockTransferService: Server created on 36379
25/12/04 11:27:47 INFO BlockManagerMaster: Trying to register BlockManager
25/12/04 11:27:47 INFO BlockManagerMasterEndpoint: Registering block manager 49.52.27.65:36379 with 143.3 MB RAM, BlockManagerId(driver, 49.52.27.65, 36379)
25/12/04 11:27:47 INFO AppClient$ClientEndpoint: Executor updated: app-20251204062737-0127/1 is now RUNNING
25/12/04 11:27:47 INFO BlockManagerMaster: Registered BlockManager
25/12/04 11:27:47 INFO AppClient$ClientEndpoint: Executor updated: app-20251204062737-0127/0 is now RUNNING
25/12/04 11:27:57 INFO EventLoggingListener: Logging events to file:/home/djk/Documents/shuffle-experiment/logs/shuffle_file_test_20251204_112735/events_hash_uniform/app-20251204062737-0127
25/12/04 11:27:57 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
=== Shuffle 实验配置 ===
Manager:  hash
Input:    file:///tmp/tpch-dbgen/gen_data/lineitem-medium.tbl
Scenario: uniform
>>> 正在预热 (Cache & Count)...
>>> 预热完成，记录数: 29999795

=== 开始运行 hash-uniform 实验 ===
25/12/04 11:29:28 WARN TaskSetManager: Lost task 2.0 in stage 4.0 (TID 220, 49.52.27.60): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.regex.Pattern$BitClass.<init>(Pattern.java:3331)
	at java.util.regex.Pattern.clazz(Pattern.java:2498)
	at java.util.regex.Pattern.sequence(Pattern.java:2077)
	at java.util.regex.Pattern.expr(Pattern.java:2010)
	at java.util.regex.Pattern.group0(Pattern.java:2919)
	at java.util.regex.Pattern.sequence(Pattern.java:2065)
	at java.util.regex.Pattern.expr(Pattern.java:2010)
	at java.util.regex.Pattern.compile(Pattern.java:1702)
	at java.util.regex.Pattern.<init>(Pattern.java:1352)
	at java.util.regex.Pattern.compile(Pattern.java:1028)
	at org.apache.spark.network.util.JavaUtils.parseByteString(JavaUtils.java:219)
	at org.apache.spark.network.util.JavaUtils.byteStringAsBytes(JavaUtils.java:255)
	at org.apache.spark.util.Utils$.byteStringAsBytes(Utils.scala:1002)
	at org.apache.spark.SparkConf.getSizeAsBytes(SparkConf.scala:249)
	at org.apache.spark.io.SnappyCompressionCodec.compressedOutputStream(CompressionCodec.scala:155)
	at org.apache.spark.storage.BlockManager.wrapForCompression(BlockManager.scala:1235)
	at org.apache.spark.storage.BlockManager$$anonfun$10.apply(BlockManager.scala:670)
	at org.apache.spark.storage.BlockManager$$anonfun$10.apply(BlockManager.scala:670)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:91)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:28 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 218, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/35/shuffle_2_0_162.c7b45502-1a11-401e-a730-7aeca0423718 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:28 WARN TaskSetManager: Lost task 4.0 in stage 4.0 (TID 221, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/06/shuffle_2_4_734.b64c8b63-33da-4331-8368-b5cdcec378d1 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:28 WARN TaskSetManager: Lost task 2.1 in stage 4.0 (TID 222, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/07/shuffle_2_2_999.06b204fe-2580-4c7a-bdbe-721f53ead8e3 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:28 WARN TaskSetManager: Lost task 5.0 in stage 4.0 (TID 223, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/07/shuffle_2_5_655.58a2137e-a618-4bdd-8baa-6df385bb32dd (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:29 WARN TaskSetManager: Lost task 4.1 in stage 4.0 (TID 224, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/06/shuffle_2_4_734.5ba78300-f0b4-47fc-ae14-39b172d6394b (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:29 WARN TaskSetManager: Lost task 0.1 in stage 4.0 (TID 225, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/19/shuffle_2_0_1611.49302896-5561-47ed-a909-eda822910914 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:29 WARN TaskSetManager: Lost task 2.2 in stage 4.0 (TID 227, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/1f/shuffle_2_2_43.bfff1b40-f612-4e95-940d-40d42b430c2c (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:29 WARN TaskSetManager: Lost task 5.1 in stage 4.0 (TID 226, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/1f/shuffle_2_5_1720.75ffcafc-abb4-4fa7-8ee0-0fdbd831c61d (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:29 WARN TaskSetManager: Lost task 4.2 in stage 4.0 (TID 229, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/03/shuffle_2_4_1000.4f6e07e2-f8f9-4889-b865-4a2b01e2160e (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:29 WARN TaskSetManager: Lost task 0.2 in stage 4.0 (TID 228, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/13/shuffle_2_0_1958.3f668614-3ec2-4740-a42e-8c4888152684 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:30 WARN TaskSetManager: Lost task 2.3 in stage 4.0 (TID 231, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/1f/shuffle_2_2_43.371718a7-a8e6-495d-952b-8eef8559196a (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 11:29:30 ERROR TaskSetManager: Task 2 in stage 4.0 failed 4 times; aborting job
25/12/04 11:29:30 WARN TaskSetManager: Lost task 5.2 in stage 4.0 (TID 230, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/1f/shuffle_2_5_1720.8047eca4-e285-45be-966e-d1703f665d38 (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 4 times, most recent failure: Lost task 2.3 in stage 4.0 (TID 231, 49.52.27.60): java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/1f/shuffle_2_2_43.371718a7-a8e6-495d-952b-8eef8559196a (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1157)
	at edu.ecnu.TPCHTest$.runExperiment(TPCHTest.scala:93)
	at edu.ecnu.TPCHTest$.run(TPCHTest.scala:63)
	at edu.ecnu.MainEntry$.main(MainEntry.scala:29)
	at edu.ecnu.MainEntry.main(MainEntry.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-13d129b0-a6cd-4581-86e0-c49b43fc9ed4/blockmgr-fbe438ef-9bbc-4585-8cb8-56ac13822feb/1f/shuffle_2_2_43.371718a7-a8e6-495d-952b-8eef8559196a (No such file or directory)
	at java.io.FileOutputStream.open0(Native Method)
	at java.io.FileOutputStream.open(FileOutputStream.java:270)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	at org.apache.spark.storage.DiskBlockObjectWriter.open(DiskBlockObjectWriter.scala:88)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:181)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:68)
	at org.apache.spark.shuffle.hash.HashShuffleWriter$$anonfun$write$1.apply(HashShuffleWriter.scala:66)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.scala:66)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

================================================================================
RESULT HEADER: Mode,Scenario,Count,Time(s),FileCount,WriteSize,DiskSpill,GCTime,Status
RESULT_CSV: hash,uniform,0,0,5611,0,0,0,Failed-SparkException
================================================================================

25/12/04 11:29:30 WARN TaskSetManager: Lost task 0.3 in stage 4.0 (TID 232, 49.52.27.60): TaskKilled (killed intentionally)
25/12/04 11:29:30 WARN TaskSetManager: Lost task 4.3 in stage 4.0 (TID 233, 49.52.27.60): TaskKilled (killed intentionally)
