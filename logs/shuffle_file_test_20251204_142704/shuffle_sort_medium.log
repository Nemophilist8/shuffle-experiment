=== Dispatcher: Routing to task [file] ===
Running File Count Monitor with args: sort medium
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/12/04 14:27:07 INFO SparkContext: Running Spark version 1.6.3
25/12/04 14:27:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/12/04 14:27:07 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
25/12/04 14:27:07 WARN Utils: Your hostname, hygon7490 resolves to a loopback address: 127.0.0.1; using 49.52.27.65 instead (on interface ens2f1np1)
25/12/04 14:27:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/12/04 14:27:07 INFO SecurityManager: Changing view acls to: djk
25/12/04 14:27:07 INFO SecurityManager: Changing modify acls to: djk
25/12/04 14:27:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(djk); users with modify permissions: Set(djk)
25/12/04 14:27:07 INFO Utils: Successfully started service 'sparkDriver' on port 46707.
25/12/04 14:27:07 INFO Slf4jLogger: Slf4jLogger started
25/12/04 14:27:07 INFO Remoting: Starting remoting
25/12/04 14:27:08 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@49.52.27.65:36069]
25/12/04 14:27:08 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 36069.
25/12/04 14:27:08 INFO SparkEnv: Registering MapOutputTracker
25/12/04 14:27:08 INFO SparkEnv: Registering BlockManagerMaster
25/12/04 14:27:08 INFO DiskBlockManager: Created local directory at /tmp/spark/work/blockmgr-b927138a-0395-44f7-8eb0-f019ce0f8bd7
25/12/04 14:27:08 INFO MemoryStore: MemoryStore started with capacity 143.3 MB
25/12/04 14:27:08 INFO SparkEnv: Registering OutputCommitCoordinator
25/12/04 14:27:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/12/04 14:27:08 INFO SparkUI: Started SparkUI at http://49.52.27.65:4040
25/12/04 14:27:08 INFO HttpFileServer: HTTP File server directory is /tmp/spark/work/spark-794c8d9a-a245-4962-bb92-b45180ed561f/httpd-51b38914-93ce-4b12-a3c2-aaba3287a990
25/12/04 14:27:08 INFO HttpServer: Starting HTTP Server
25/12/04 14:27:08 INFO Utils: Successfully started service 'HTTP file server' on port 35585.
25/12/04 14:27:08 INFO SparkContext: Added JAR file:/home/djk/Documents/shuffle-experiment/target/scala-2.10/spark-shuffle-experiment-1.0.0.jar at http://49.52.27.65:35585/jars/spark-shuffle-experiment-1.0.0.jar with timestamp 1764858428597
25/12/04 14:27:08 INFO AppClient$ClientEndpoint: Connecting to master spark://49.52.27.113:7077...
25/12/04 14:27:18 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20251204092708-0151
25/12/04 14:27:18 INFO AppClient$ClientEndpoint: Executor added: app-20251204092708-0151/0 on worker-20251203224515-49.52.27.60-35123 (49.52.27.60:35123) with 2 cores
25/12/04 14:27:18 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204092708-0151/0 on hostPort 49.52.27.60:35123 with 2 cores, 4.0 GB RAM
25/12/04 14:27:18 INFO AppClient$ClientEndpoint: Executor added: app-20251204092708-0151/1 on worker-20251203144508-49.52.27.65-34725 (49.52.27.65:34725) with 2 cores
25/12/04 14:27:18 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204092708-0151/1 on hostPort 49.52.27.65:34725 with 2 cores, 4.0 GB RAM
25/12/04 14:27:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37121.
25/12/04 14:27:18 INFO NettyBlockTransferService: Server created on 37121
25/12/04 14:27:18 INFO BlockManagerMaster: Trying to register BlockManager
25/12/04 14:27:18 INFO BlockManagerMasterEndpoint: Registering block manager 49.52.27.65:37121 with 143.3 MB RAM, BlockManagerId(driver, 49.52.27.65, 37121)
25/12/04 14:27:18 INFO BlockManagerMaster: Registered BlockManager
25/12/04 14:27:18 INFO AppClient$ClientEndpoint: Executor updated: app-20251204092708-0151/0 is now RUNNING
25/12/04 14:27:18 INFO AppClient$ClientEndpoint: Executor updated: app-20251204092708-0151/1 is now RUNNING
25/12/04 14:27:29 INFO EventLoggingListener: Logging events to file:/tmp/spark-events/app-20251204092708-0151
25/12/04 14:27:29 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
=== Shuffle File Monitor 实验 ===
3. Spark版本: 1.6.3
默认shuffle分区数: 200
请求的Shuffle Manager: sort
当前 Shuffle Manager: sort
Spark 本地目录: /tmp/spark/work
监控将每秒检查一次 /tmp/spark/work 目录中的 shuffle_* 文件
测试数据集: medium
分区数: 100
>>> 正在进行数据预热 (Cache & Count)...
>>> 预热完成! 数据已驻留内存。耗时: 27658ms, 记录数: 5000000
>>> 现在开始正式 Shuffle 实验 (此时文件应该会立即产生)

=== 开始运行 sort 实验 ===
启动Shuffle文件监控，每秒检查一次...
[ShuffleMonitor 14:27:57] 文件数: 96, 总大小: 1KB (峰值: 96文件, 1KB)
[ShuffleMonitor 14:27:58] 文件数: 96, 总大小: 1KB (峰值: 96文件, 1KB)
[ShuffleMonitor 14:27:59] 文件数: 100, 总大小: 148.57MB (峰值: 100文件, 148.57MB)
[ShuffleMonitor 14:28:01] 文件数: 104, 总大小: 297.32MB (峰值: 104文件, 297.32MB)
[ShuffleMonitor 14:28:02] 文件数: 109, 总大小: 460.92MB (峰值: 109文件, 460.92MB)
[ShuffleMonitor 14:28:03] 文件数: 113, 总大小: 637.16MB (峰值: 113文件, 637.16MB)
[ShuffleMonitor 14:28:04] 文件数: 117, 总大小: 744.55MB (峰值: 117文件, 744.55MB)
[ShuffleMonitor 14:28:05] 文件数: 119, 总大小: 887.59MB (峰值: 119文件, 887.59MB)
[ShuffleMonitor 14:28:06] 文件数: 123, 总大小: 1001.82MB (峰值: 123文件, 1001.82MB)
[ShuffleMonitor 14:28:07] 文件数: 127, 总大小: 1.11GB (峰值: 127文件, 1.11GB)
[ShuffleMonitor 14:28:08] 文件数: 130, 总大小: 1.23GB (峰值: 130文件, 1.23GB)
[ShuffleMonitor 14:28:09] 文件数: 134, 总大小: 1.38GB (峰值: 134文件, 1.38GB)
[ShuffleMonitor 14:28:10] 文件数: 138, 总大小: 1.52GB (峰值: 138文件, 1.52GB)
[ShuffleMonitor 14:28:11] 文件数: 142, 总大小: 1.67GB (峰值: 142文件, 1.67GB)
[ShuffleMonitor 14:28:12] 文件数: 144, 总大小: 1.73GB (峰值: 144文件, 1.73GB)
[ShuffleMonitor 14:28:13] 文件数: 148, 总大小: 1.85GB (峰值: 148文件, 1.85GB)
[ShuffleMonitor 14:28:14] 文件数: 152, 总大小: 2.00GB (峰值: 152文件, 2.00GB)
[ShuffleMonitor 14:28:15] 文件数: 154, 总大小: 2.10GB (峰值: 154文件, 2.10GB)
[ShuffleMonitor 14:28:16] 文件数: 156, 总大小: 2.11GB (峰值: 156文件, 2.11GB)
[ShuffleMonitor 14:28:17] 文件数: 159, 总大小: 2.25GB (峰值: 159文件, 2.25GB)
[ShuffleMonitor 14:28:18] 文件数: 162, 总大小: 2.39GB (峰值: 162文件, 2.39GB)
[ShuffleMonitor 14:28:19] 文件数: 166, 总大小: 2.54GB (峰值: 166文件, 2.54GB)
[ShuffleMonitor 14:28:20] 文件数: 170, 总大小: 2.68GB (峰值: 170文件, 2.68GB)
[ShuffleMonitor 14:28:21] 文件数: 174, 总大小: 2.83GB (峰值: 174文件, 2.83GB)
[ShuffleMonitor 14:28:22] 文件数: 178, 总大小: 2.97GB (峰值: 178文件, 2.97GB)
[ShuffleMonitor 14:28:23] 文件数: 181, 总大小: 3.11GB (峰值: 181文件, 3.11GB)
[ShuffleMonitor 14:28:24] 文件数: 187, 总大小: 3.27GB (峰值: 187文件, 3.27GB)
[ShuffleMonitor 14:28:25] 文件数: 191, 总大小: 3.43GB (峰值: 191文件, 3.43GB)
[ShuffleMonitor 14:28:26] 文件数: 195, 总大小: 3.59GB (峰值: 195文件, 3.59GB)
[ShuffleMonitor 14:28:27] 文件数: 199, 总大小: 3.76GB (峰值: 199文件, 3.76GB)
[ShuffleMonitor 14:28:28] 文件数: 204, 总大小: 3.92GB (峰值: 204文件, 3.92GB)
[ShuffleMonitor 14:28:29] 文件数: 208, 总大小: 4.06GB (峰值: 208文件, 4.06GB)
[ShuffleMonitor 14:28:30] 文件数: 211, 总大小: 4.20GB (峰值: 211文件, 4.20GB)
[ShuffleMonitor 14:28:31] 文件数: 215, 总大小: 4.34GB (峰值: 215文件, 4.34GB)
[ShuffleMonitor 14:28:32] 文件数: 219, 总大小: 4.48GB (峰值: 219文件, 4.48GB)
[ShuffleMonitor 14:28:33] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:34] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:35] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:36] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:37] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:38] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:39] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:40] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:41] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:42] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:43] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:44] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:45] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:46] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:47] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:48] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:49] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:50] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:51] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:52] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:53] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:54] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:55] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:56] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:57] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:58] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:28:59] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:00] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:01] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:02] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:03] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:04] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:05] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:06] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:07] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:08] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:09] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:10] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:11] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:12] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:13] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:14] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:15] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:16] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:17] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:18] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:19] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:20] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:21] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:22] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:23] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:24] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:25] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:26] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:27] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:28] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:29] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:30] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:31] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:32] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:33] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:34] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:35] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:36] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:37] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:38] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:39] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:40] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:41] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:42] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:43] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:44] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:45] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:46] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:47] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:48] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
25/12/04 14:29:48 WARN TaskSetManager: Lost task 437.0 in stage 3.0 (TID 638, 49.52.27.60): java.lang.OutOfMemoryError: GC overhead limit exceeded

[ShuffleMonitor 14:29:49] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
25/12/04 14:29:50 ERROR TaskSchedulerImpl: Lost executor 0 on 49.52.27.60: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 14:29:50 WARN TaskSetManager: Lost task 435.0 in stage 3.0 (TID 636, 49.52.27.60): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
[ShuffleMonitor 14:29:50] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:51] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:52] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:53] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:54] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:55] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:56] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:57] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:58] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:29:59] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:00] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:01] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:02] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:03] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:04] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:05] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:06] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:07] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:09] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:10] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:11] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:12] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:13] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:14] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:15] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
25/12/04 14:30:15 WARN TaskSetManager: Lost task 435.1 in stage 3.0 (TID 703, 49.52.27.60): FetchFailed(null, shuffleId=1, mapId=-1, reduceId=435, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:548)
	at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$2.apply(MapOutputTracker.scala:544)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:544)
	at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:155)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:47)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

)
[ShuffleMonitor 14:30:16] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:17] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:18] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:19] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:20] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:21] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:22] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
25/12/04 14:30:22 WARN TaskSetManager: Lost task 436.0 in stage 3.0 (TID 637, 49.52.27.65): FetchFailed(BlockManagerId(0, 49.52.27.60, 37477), shuffleId=1, mapId=2, reduceId=436, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:37477
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:37477
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:37477
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
[ShuffleMonitor 14:30:23] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:24] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:25] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:26] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:27] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:28] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:29] 文件数: 220, 总大小: 4.50GB (峰值: 220文件, 4.50GB)
[ShuffleMonitor 14:30:30] 文件数: 222, 总大小: 4.57GB (峰值: 222文件, 4.57GB)
25/12/04 14:30:31 WARN TaskSetManager: Lost task 437.2 in stage 3.0 (TID 702, 49.52.27.65): FetchFailed(BlockManagerId(0, 49.52.27.60, 37477), shuffleId=1, mapId=51, reduceId=437, message=
org.apache.spark.shuffle.FetchFailedException: java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-1700c68f-73dd-4cea-80e9-a30c58c6ee36/blockmgr-2de823bb-43ee-4686-bd2b-41c0ed0f4013/05/shuffle_1_51_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:197)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:298)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:58)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:58)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:58)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:149)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:750)

	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.RuntimeException: java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-1700c68f-73dd-4cea-80e9-a30c58c6ee36/blockmgr-2de823bb-43ee-4686-bd2b-41c0ed0f4013/05/shuffle_1_51_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:197)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:298)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:58)
	at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$2.apply(NettyBlockRpcServer.scala:58)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:58)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:149)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:102)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:104)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:750)

	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:186)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:106)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:86)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
[ShuffleMonitor 14:30:31] 文件数: 223, 总大小: 4.60GB (峰值: 223文件, 4.60GB)
[ShuffleMonitor 14:30:32] 文件数: 226, 总大小: 4.69GB (峰值: 226文件, 4.69GB)
[ShuffleMonitor 14:30:33] 文件数: 228, 总大小: 4.79GB (峰值: 228文件, 4.79GB)
[ShuffleMonitor 14:30:34] 文件数: 230, 总大小: 4.89GB (峰值: 230文件, 4.89GB)
[ShuffleMonitor 14:30:35] 文件数: 234, 总大小: 5.00GB (峰值: 234文件, 5.00GB)
[ShuffleMonitor 14:30:36] 文件数: 238, 总大小: 5.14GB (峰值: 238文件, 5.14GB)
[ShuffleMonitor 14:30:37] 文件数: 242, 总大小: 5.28GB (峰值: 242文件, 5.28GB)
[ShuffleMonitor 14:30:38] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:39] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:40] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:41] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:42] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:43] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:44] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:45] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:46] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:47] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:48] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:49] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:50] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:51] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:52] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:53] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:54] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:55] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:56] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
[ShuffleMonitor 14:30:57] 文件数: 244, 总大小: 5.37GB (峰值: 244文件, 5.37GB)
