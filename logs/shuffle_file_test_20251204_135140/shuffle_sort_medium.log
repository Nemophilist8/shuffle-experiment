=== Dispatcher: Routing to task [file] ===
Running File Count Monitor with args: sort medium
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/12/04 13:51:42 INFO SparkContext: Running Spark version 1.6.3
25/12/04 13:51:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/12/04 13:51:43 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
25/12/04 13:51:43 WARN Utils: Your hostname, hygon7490 resolves to a loopback address: 127.0.0.1; using 49.52.27.65 instead (on interface ens2f1np1)
25/12/04 13:51:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/12/04 13:51:43 INFO SecurityManager: Changing view acls to: djk
25/12/04 13:51:43 INFO SecurityManager: Changing modify acls to: djk
25/12/04 13:51:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(djk); users with modify permissions: Set(djk)
25/12/04 13:51:43 INFO Utils: Successfully started service 'sparkDriver' on port 38031.
25/12/04 13:51:43 INFO Slf4jLogger: Slf4jLogger started
25/12/04 13:51:43 INFO Remoting: Starting remoting
25/12/04 13:51:43 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@49.52.27.65:42631]
25/12/04 13:51:43 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 42631.
25/12/04 13:51:43 INFO SparkEnv: Registering MapOutputTracker
25/12/04 13:51:43 INFO SparkEnv: Registering BlockManagerMaster
25/12/04 13:51:43 INFO DiskBlockManager: Created local directory at /tmp/spark/work/blockmgr-c1934b8e-1f38-4d94-8d73-7e36f431d27d
25/12/04 13:51:43 INFO MemoryStore: MemoryStore started with capacity 143.3 MB
25/12/04 13:51:44 INFO SparkEnv: Registering OutputCommitCoordinator
25/12/04 13:51:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/12/04 13:51:44 INFO SparkUI: Started SparkUI at http://49.52.27.65:4040
25/12/04 13:51:44 INFO HttpFileServer: HTTP File server directory is /tmp/spark/work/spark-5d8aefbe-4a0b-449e-b27c-dd6f4fc7cada/httpd-c976b456-e20a-441a-aaf5-604a7c0f9f5d
25/12/04 13:51:44 INFO HttpServer: Starting HTTP Server
25/12/04 13:51:44 INFO Utils: Successfully started service 'HTTP file server' on port 44603.
25/12/04 13:51:44 INFO SparkContext: Added JAR file:/home/djk/Documents/shuffle-experiment/target/scala-2.10/spark-shuffle-experiment-1.0.0.jar at http://49.52.27.65:44603/jars/spark-shuffle-experiment-1.0.0.jar with timestamp 1764856304489
25/12/04 13:51:44 INFO AppClient$ClientEndpoint: Connecting to master spark://49.52.27.113:7077...
25/12/04 13:51:51 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20251204085144-0145
25/12/04 13:51:51 INFO AppClient$ClientEndpoint: Executor added: app-20251204085144-0145/0 on worker-20251203224515-49.52.27.60-35123 (49.52.27.60:35123) with 2 cores
25/12/04 13:51:51 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204085144-0145/0 on hostPort 49.52.27.60:35123 with 2 cores, 1024.0 MB RAM
25/12/04 13:51:51 INFO AppClient$ClientEndpoint: Executor added: app-20251204085144-0145/1 on worker-20251203144508-49.52.27.65-34725 (49.52.27.65:34725) with 2 cores
25/12/04 13:51:51 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204085144-0145/1 on hostPort 49.52.27.65:34725 with 2 cores, 1024.0 MB RAM
25/12/04 13:51:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37039.
25/12/04 13:51:51 INFO NettyBlockTransferService: Server created on 37039
25/12/04 13:51:51 INFO BlockManagerMaster: Trying to register BlockManager
25/12/04 13:51:51 INFO BlockManagerMasterEndpoint: Registering block manager 49.52.27.65:37039 with 143.3 MB RAM, BlockManagerId(driver, 49.52.27.65, 37039)
25/12/04 13:51:51 INFO AppClient$ClientEndpoint: Executor updated: app-20251204085144-0145/1 is now RUNNING
25/12/04 13:51:51 INFO AppClient$ClientEndpoint: Executor updated: app-20251204085144-0145/0 is now RUNNING
25/12/04 13:51:51 INFO BlockManagerMaster: Registered BlockManager
25/12/04 13:52:02 INFO EventLoggingListener: Logging events to file:/tmp/spark-events/app-20251204085144-0145
25/12/04 13:52:02 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
=== Shuffle File Monitor 实验 ===
3. Spark版本: 1.6.3
默认shuffle分区数: 200
请求的Shuffle Manager: sort
当前 Shuffle Manager: sort
Spark 本地目录: /tmp/spark/work
监控将每秒检查一次 /tmp/spark/work 目录中的 shuffle_* 文件
测试数据集: medium
分区数: 100
>>> 正在进行数据预热 (Cache & Count)...
>>> 预热完成! 数据已驻留内存。耗时: 27207ms, 记录数: 5000000
>>> 现在开始正式 Shuffle 实验 (此时文件应该会立即产生)

=== 开始运行 sort 实验 ===
启动Shuffle文件监控，每秒检查一次...
[ShuffleMonitor 13:52:30] 文件数: 94, 总大小: 1KB (峰值: 94文件, 1KB)
[ShuffleMonitor 13:52:31] 文件数: 96, 总大小: 3.15MB (峰值: 96文件, 3.15MB)
[ShuffleMonitor 13:52:32] 文件数: 98, 总大小: 147.81MB (峰值: 98文件, 147.81MB)
[ShuffleMonitor 13:52:33] 文件数: 100, 总大小: 158.94MB (峰值: 100文件, 158.94MB)
[ShuffleMonitor 13:52:34] 文件数: 102, 总大小: 296.18MB (峰值: 102文件, 296.18MB)
[ShuffleMonitor 13:52:35] 文件数: 105, 总大小: 437.38MB (峰值: 105文件, 437.38MB)
[ShuffleMonitor 13:52:36] 文件数: 106, 总大小: 444.48MB (峰值: 106文件, 444.48MB)
[ShuffleMonitor 13:52:37] 文件数: 110, 总大小: 592.83MB (峰值: 110文件, 592.83MB)
[ShuffleMonitor 13:52:38] 文件数: 111, 总大小: 619.00MB (峰值: 111文件, 619.00MB)
[ShuffleMonitor 13:52:39] 文件数: 115, 总大小: 781.64MB (峰值: 115文件, 781.64MB)
[ShuffleMonitor 13:52:40] 文件数: 117, 总大小: 866.11MB (峰值: 117文件, 866.11MB)
[ShuffleMonitor 13:52:41] 文件数: 119, 总大小: 917.51MB (峰值: 119文件, 917.51MB)
[ShuffleMonitor 13:52:42] 文件数: 123, 总大小: 1.03GB (峰值: 123文件, 1.03GB)
[ShuffleMonitor 13:52:43] 文件数: 123, 总大小: 1.04GB (峰值: 123文件, 1.04GB)
[ShuffleMonitor 13:52:44] 文件数: 126, 总大小: 1.15GB (峰值: 126文件, 1.15GB)
[ShuffleMonitor 13:52:45] 文件数: 128, 总大小: 1.23GB (峰值: 128文件, 1.23GB)
[ShuffleMonitor 13:52:46] 文件数: 129, 总大小: 1.28GB (峰值: 129文件, 1.28GB)
[ShuffleMonitor 13:52:47] 文件数: 133, 总大小: 1.38GB (峰值: 133文件, 1.38GB)
[ShuffleMonitor 13:52:48] 文件数: 133, 总大小: 1.42GB (峰值: 133文件, 1.42GB)
[ShuffleMonitor 13:52:49] 文件数: 136, 总大小: 1.52GB (峰值: 136文件, 1.52GB)
[ShuffleMonitor 13:52:50] 文件数: 139, 总大小: 1.63GB (峰值: 139文件, 1.63GB)
[ShuffleMonitor 13:52:51] 文件数: 140, 总大小: 1.66GB (峰值: 140文件, 1.66GB)
[ShuffleMonitor 13:52:52] 文件数: 143, 总大小: 1.77GB (峰值: 143文件, 1.77GB)
[ShuffleMonitor 13:52:53] 文件数: 144, 总大小: 1.81GB (峰值: 144文件, 1.81GB)
[ShuffleMonitor 13:52:54] 文件数: 146, 总大小: 1.83GB (峰值: 146文件, 1.83GB)
[ShuffleMonitor 13:52:55] 文件数: 148, 总大小: 1.95GB (峰值: 148文件, 1.95GB)
[ShuffleMonitor 13:52:56] 文件数: 151, 总大小: 2.09GB (峰值: 151文件, 2.09GB)
[ShuffleMonitor 13:52:57] 文件数: 152, 总大小: 2.10GB (峰值: 152文件, 2.10GB)
[ShuffleMonitor 13:52:58] 文件数: 155, 总大小: 2.24GB (峰值: 155文件, 2.24GB)
[ShuffleMonitor 13:52:59] 文件数: 157, 总大小: 2.24GB (峰值: 157文件, 2.24GB)
[ShuffleMonitor 13:53:00] 文件数: 159, 总大小: 2.33GB (峰值: 159文件, 2.33GB)
[ShuffleMonitor 13:53:01] 文件数: 161, 总大小: 2.44GB (峰值: 161文件, 2.44GB)
[ShuffleMonitor 13:53:02] 文件数: 164, 总大小: 2.53GB (峰值: 164文件, 2.53GB)
[ShuffleMonitor 13:53:03] 文件数: 165, 总大小: 2.58GB (峰值: 165文件, 2.58GB)
[ShuffleMonitor 13:53:04] 文件数: 169, 总大小: 2.72GB (峰值: 169文件, 2.72GB)
[ShuffleMonitor 13:53:05] 文件数: 171, 总大小: 2.76GB (峰值: 171文件, 2.76GB)
[ShuffleMonitor 13:53:06] 文件数: 173, 总大小: 2.84GB (峰值: 173文件, 2.84GB)
[ShuffleMonitor 13:53:07] 文件数: 176, 总大小: 2.95GB (峰值: 176文件, 2.95GB)
[ShuffleMonitor 13:53:08] 文件数: 177, 总大小: 2.99GB (峰值: 177文件, 2.99GB)
[ShuffleMonitor 13:53:10] 文件数: 179, 总大小: 3.06GB (峰值: 179文件, 3.06GB)
[ShuffleMonitor 13:53:11] 文件数: 182, 总大小: 3.18GB (峰值: 182文件, 3.18GB)
[ShuffleMonitor 13:53:12] 文件数: 183, 总大小: 3.21GB (峰值: 183文件, 3.21GB)
[ShuffleMonitor 13:53:13] 文件数: 186, 总大小: 3.33GB (峰值: 186文件, 3.33GB)
[ShuffleMonitor 13:53:14] 文件数: 189, 总大小: 3.42GB (峰值: 189文件, 3.42GB)
[ShuffleMonitor 13:53:15] 文件数: 190, 总大小: 3.47GB (峰值: 190文件, 3.47GB)
[ShuffleMonitor 13:53:16] 文件数: 193, 总大小: 3.58GB (峰值: 193文件, 3.58GB)
[ShuffleMonitor 13:53:17] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:18] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:19] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:20] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:21] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:22] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:23] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:24] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:25] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:26] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:27] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:28] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:29] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:30] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:31] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:32] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:33] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:34] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:35] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:36] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:37] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:38] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:39] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:40] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:41] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:42] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:43] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:44] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:45] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:46] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:47] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:48] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:49] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:50] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:51] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:52] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:53] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:54] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:55] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:56] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:57] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:58] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:53:59] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:00] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:01] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:02] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:03] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:04] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:05] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:06] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:07] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:08] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:09] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:10] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:11] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:12] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:13] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:14] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:15] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:16] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:17] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:18] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:19] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:20] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
25/12/04 13:54:21 WARN TaskSetManager: Lost task 436.0 in stage 3.0 (TID 637, 49.52.27.60): java.lang.OutOfMemoryError: GC overhead limit exceeded

25/12/04 13:54:21 WARN TaskSetManager: Lost task 436.1 in stage 3.0 (TID 701, 49.52.27.60): FetchFailed(BlockManagerId(0, 49.52.27.60, 42751), shuffleId=1, mapId=1, reduceId=436, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-1f3ee3ef-ed24-4356-a096-9a6e4ae0b32e/blockmgr-c783b56a-f247-49fd-aaf8-5a05ba703e52/32/shuffle_1_1_0.index (No such file or directory)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-1f3ee3ef-ed24-4356-a096-9a6e4ae0b32e/blockmgr-c783b56a-f247-49fd-aaf8-5a05ba703e52/32/shuffle_1_1_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:197)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:298)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:238)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:112)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:43)
	... 9 more

)
[ShuffleMonitor 13:54:21] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
25/12/04 13:54:22 ERROR TaskSchedulerImpl: Lost executor 0 on 49.52.27.60: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:54:22 WARN TaskSetManager: Lost task 0.0 in stage 2.1 (TID 702, 49.52.27.60): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:54:22 WARN TaskSetManager: Lost task 1.0 in stage 2.1 (TID 703, 49.52.27.60): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
[ShuffleMonitor 13:54:22] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:23] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:24] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:25] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:26] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:27] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:28] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:29] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:30] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:31] 文件数: 194, 总大小: 3.62GB (峰值: 194文件, 3.62GB)
[ShuffleMonitor 13:54:32] 文件数: 196, 总大小: 3.62GB (峰值: 196文件, 3.62GB)
[ShuffleMonitor 13:54:33] 文件数: 196, 总大小: 3.62GB (峰值: 196文件, 3.62GB)
[ShuffleMonitor 13:54:34] 文件数: 196, 总大小: 3.62GB (峰值: 196文件, 3.62GB)
[ShuffleMonitor 13:54:35] 文件数: 196, 总大小: 3.62GB (峰值: 196文件, 3.62GB)
[ShuffleMonitor 13:54:36] 文件数: 196, 总大小: 3.62GB (峰值: 196文件, 3.62GB)
25/12/04 13:54:37 WARN TaskSetManager: Lost task 437.0 in stage 3.0 (TID 638, 49.52.27.65): FetchFailed(BlockManagerId(0, 49.52.27.60, 42751), shuffleId=1, mapId=43, reduceId=437, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:42751
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:42751
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:42751
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
25/12/04 13:54:37 WARN TaskSetManager: Lost task 435.0 in stage 3.0 (TID 636, 49.52.27.65): FetchFailed(BlockManagerId(0, 49.52.27.60, 42751), shuffleId=1, mapId=10, reduceId=435, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:42751
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:42751
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:42751
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
[ShuffleMonitor 13:54:37] 文件数: 196, 总大小: 3.62GB (峰值: 196文件, 3.62GB)
[ShuffleMonitor 13:54:38] 文件数: 198, 总大小: 3.62GB (峰值: 198文件, 3.62GB)
[ShuffleMonitor 13:54:39] 文件数: 200, 总大小: 3.76GB (峰值: 200文件, 3.76GB)
[ShuffleMonitor 13:54:40] 文件数: 201, 总大小: 3.77GB (峰值: 201文件, 3.77GB)
[ShuffleMonitor 13:54:41] 文件数: 202, 总大小: 3.88GB (峰值: 202文件, 3.88GB)
[ShuffleMonitor 13:54:42] 文件数: 206, 总大小: 3.95GB (峰值: 206文件, 3.95GB)
[ShuffleMonitor 13:54:43] 文件数: 208, 总大小: 4.05GB (峰值: 208文件, 4.05GB)
[ShuffleMonitor 13:54:44] 文件数: 212, 总大小: 4.19GB (峰值: 212文件, 4.19GB)
[ShuffleMonitor 13:54:45] 文件数: 213, 总大小: 4.20GB (峰值: 213文件, 4.20GB)
[ShuffleMonitor 13:54:46] 文件数: 216, 总大小: 4.34GB (峰值: 216文件, 4.34GB)
[ShuffleMonitor 13:54:47] 文件数: 219, 总大小: 4.46GB (峰值: 219文件, 4.46GB)
[ShuffleMonitor 13:54:48] 文件数: 220, 总大小: 4.48GB (峰值: 220文件, 4.48GB)
[ShuffleMonitor 13:54:49] 文件数: 224, 总大小: 4.63GB (峰值: 224文件, 4.63GB)
[ShuffleMonitor 13:54:50] 文件数: 224, 总大小: 4.63GB (峰值: 224文件, 4.63GB)
[ShuffleMonitor 13:54:51] 文件数: 228, 总大小: 4.77GB (峰值: 228文件, 4.77GB)
[ShuffleMonitor 13:54:52] 文件数: 229, 总大小: 4.79GB (峰值: 229文件, 4.79GB)
[ShuffleMonitor 13:54:53] 文件数: 231, 总大小: 4.90GB (峰值: 231文件, 4.90GB)
[ShuffleMonitor 13:54:54] 文件数: 233, 总大小: 4.95GB (峰值: 233文件, 4.95GB)
[ShuffleMonitor 13:54:55] 文件数: 235, 总大小: 5.02GB (峰值: 235文件, 5.02GB)
[ShuffleMonitor 13:54:56] 文件数: 237, 总大小: 5.11GB (峰值: 237文件, 5.11GB)
[ShuffleMonitor 13:54:57] 文件数: 239, 总大小: 5.20GB (峰值: 239文件, 5.20GB)
[ShuffleMonitor 13:54:58] 文件数: 241, 总大小: 5.27GB (峰值: 241文件, 5.27GB)
[ShuffleMonitor 13:54:59] 文件数: 245, 总大小: 5.35GB (峰值: 245文件, 5.35GB)
[ShuffleMonitor 13:55:00] 文件数: 245, 总大小: 5.38GB (峰值: 245文件, 5.38GB)
[ShuffleMonitor 13:55:01] 文件数: 249, 总大小: 5.51GB (峰值: 249文件, 5.51GB)
[ShuffleMonitor 13:55:02] 文件数: 250, 总大小: 5.57GB (峰值: 250文件, 5.57GB)
[ShuffleMonitor 13:55:03] 文件数: 253, 总大小: 5.66GB (峰值: 253文件, 5.66GB)
[ShuffleMonitor 13:55:04] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:05] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:06] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:07] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:09] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:10] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:11] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:12] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:13] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:14] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:15] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:16] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:17] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:18] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:19] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:20] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:21] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:22] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:23] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:24] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:25] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:26] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:27] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:28] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:29] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
[ShuffleMonitor 13:55:30] 文件数: 256, 总大小: 5.78GB (峰值: 256文件, 5.78GB)
25/12/04 13:56:20 WARN TaskSetManager: Lost task 1.0 in stage 3.1 (TID 755, 49.52.27.60): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.Arrays.copyOfRange(Arrays.java:3664)
	at java.lang.String.<init>(String.java:207)
	at com.esotericsoftware.kryo.io.Input.readString(Input.java:448)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$StringArraySerializer.read(DefaultArraySerializers.java:282)
	at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$StringArraySerializer.read(DefaultArraySerializers.java:262)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:648)
	at com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.read(FieldSerializer.java:605)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:221)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:228)
	at org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:171)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator.readNextItem(ExternalAppendOnlyMap.scala:478)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap$DiskMapIterator.hasNext(ExternalAppendOnlyMap.scala:498)
	at scala.collection.Iterator$$anon$1.hasNext(Iterator.scala:847)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.org$apache$spark$util$collection$ExternalAppendOnlyMap$ExternalIterator$$readNextHashCode(ExternalAppendOnlyMap.scala:299)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator$$anonfun$next$1.apply(ExternalAppendOnlyMap.scala:372)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator$$anonfun$next$1.apply(ExternalAppendOnlyMap.scala:370)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:370)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap$ExternalIterator.next(ExternalAppendOnlyMap.scala:265)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1633)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 13:56:20 WARN TaskSetManager: Lost task 1.1 in stage 3.1 (TID 757, 49.52.27.60): FetchFailed(BlockManagerId(2, 49.52.27.60, 41961), shuffleId=1, mapId=18, reduceId=436, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-1f3ee3ef-ed24-4356-a096-9a6e4ae0b32e/blockmgr-3994f27c-0d02-4663-9e3a-76f30ac4275d/30/shuffle_1_18_0.index (No such file or directory)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-1f3ee3ef-ed24-4356-a096-9a6e4ae0b32e/blockmgr-3994f27c-0d02-4663-9e3a-76f30ac4275d/30/shuffle_1_18_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:197)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:298)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:238)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:112)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:43)
	... 9 more

)
25/12/04 13:56:20 WARN TransportChannelHandler: Exception in connection from 49.52.27.60/49.52.27.60:38214
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:313)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:881)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:242)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:750)
25/12/04 13:56:20 ERROR TaskSchedulerImpl: Lost executor 2 on 49.52.27.60: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:56:20 WARN TaskSetManager: Lost task 1.0 in stage 2.2 (TID 759, 49.52.27.60): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:56:20 WARN TaskSetManager: Lost task 0.0 in stage 2.2 (TID 758, 49.52.27.60): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:56:36 WARN TaskSetManager: Lost task 2.0 in stage 3.1 (TID 756, 49.52.27.65): FetchFailed(BlockManagerId(2, 49.52.27.60, 41961), shuffleId=1, mapId=72, reduceId=437, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:41961
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:41961
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:41961
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
25/12/04 13:56:36 WARN TaskSetManager: Lost task 0.0 in stage 3.1 (TID 754, 49.52.27.65): FetchFailed(BlockManagerId(2, 49.52.27.60, 41961), shuffleId=1, mapId=18, reduceId=435, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:41961
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:41961
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:41961
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
25/12/04 13:57:17 WARN TaskSetManager: Lost task 2.0 in stage 3.2 (TID 782, 49.52.27.60): java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.Arrays.copyOfRange(Arrays.java:3664)
	at java.lang.String.<init>(String.java:207)
	at com.esotericsoftware.kryo.io.Input.readString(Input.java:448)
	at com.esotericsoftware.kryo.serializers.DefaultSerializers$StringSerializer.read(DefaultSerializers.java:157)
	at com.esotericsoftware.kryo.serializers.DefaultSerializers$StringSerializer.read(DefaultSerializers.java:146)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:228)
	at org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:171)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:201)
	at org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:198)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

25/12/04 13:57:17 WARN TaskSetManager: Lost task 2.1 in stage 3.2 (TID 783, 49.52.27.60): FetchFailed(BlockManagerId(3, 49.52.27.60, 38927), shuffleId=1, mapId=66, reduceId=437, message=
org.apache.spark.shuffle.FetchFailedException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-1f3ee3ef-ed24-4356-a096-9a6e4ae0b32e/blockmgr-80935988-c3d6-4a5f-9462-b442dc15f773/29/shuffle_1_66_0.index (No such file or directory)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.FileNotFoundException: /tmp/spark/work/spark-8a7f7c7e-f3b8-4789-8bf7-16307955151b/executor-1f3ee3ef-ed24-4356-a096-9a6e4ae0b32e/blockmgr-80935988-c3d6-4a5f-9462-b442dc15f773/29/shuffle_1_66_0.index (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:197)
	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:298)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:238)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:269)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:112)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:43)
	... 9 more

)
25/12/04 13:57:18 WARN TransportChannelHandler: Exception in connection from 49.52.27.60/49.52.27.60:53062
java.io.IOException: Connection reset by peer
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:192)
	at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)
	at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:313)
	at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:881)
	at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:242)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:119)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:750)
25/12/04 13:57:18 ERROR TaskSchedulerImpl: Lost executor 3 on 49.52.27.60: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:57:18 WARN TaskSetManager: Lost task 1.0 in stage 2.3 (TID 785, 49.52.27.60): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:57:18 WARN TaskSetManager: Lost task 0.0 in stage 2.3 (TID 784, 49.52.27.60): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
25/12/04 13:57:54 WARN TaskSetManager: Lost task 0.0 in stage 3.2 (TID 780, 49.52.27.65): FetchFailed(BlockManagerId(3, 49.52.27.60, 38927), shuffleId=1, mapId=96, reduceId=435, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:38927
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:38927
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:38927
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
25/12/04 13:57:54 WARN TaskSetManager: Lost task 1.0 in stage 3.2 (TID 781, 49.52.27.65): FetchFailed(BlockManagerId(3, 49.52.27.60, 38927), shuffleId=1, mapId=96, reduceId=436, message=
org.apache.spark.shuffle.FetchFailedException: Failed to connect to /49.52.27.60:38927
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:323)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:300)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:51)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
	at org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:152)
	at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:45)
	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:89)
	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:98)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.IOException: Failed to connect to /49.52.27.60:38927
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: /49.52.27.60:38927
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

)
