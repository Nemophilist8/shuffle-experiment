=== Dispatcher: Routing to task [skew] ===
Running Skew Experiment with args: hash large 0.0
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
25/12/04 11:45:13 INFO SparkContext: Running Spark version 1.6.3
25/12/04 11:45:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/12/04 11:45:13 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
25/12/04 11:45:13 WARN Utils: Your hostname, hygon7490 resolves to a loopback address: 127.0.0.1; using 49.52.27.65 instead (on interface ens2f1np1)
25/12/04 11:45:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
25/12/04 11:45:13 INFO SecurityManager: Changing view acls to: djk
25/12/04 11:45:13 INFO SecurityManager: Changing modify acls to: djk
25/12/04 11:45:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(djk); users with modify permissions: Set(djk)
25/12/04 11:45:14 INFO Utils: Successfully started service 'sparkDriver' on port 34581.
25/12/04 11:45:14 INFO Slf4jLogger: Slf4jLogger started
25/12/04 11:45:14 INFO Remoting: Starting remoting
25/12/04 11:45:14 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@49.52.27.65:34391]
25/12/04 11:45:14 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 34391.
25/12/04 11:45:14 INFO SparkEnv: Registering MapOutputTracker
25/12/04 11:45:14 INFO SparkEnv: Registering BlockManagerMaster
25/12/04 11:45:14 INFO DiskBlockManager: Created local directory at /tmp/spark/work/blockmgr-694e6c2a-af65-495f-8288-e9515db976ed
25/12/04 11:45:14 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
25/12/04 11:45:14 INFO SparkEnv: Registering OutputCommitCoordinator
25/12/04 11:45:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/12/04 11:45:14 INFO SparkUI: Started SparkUI at http://49.52.27.65:4040
25/12/04 11:45:14 INFO HttpFileServer: HTTP File server directory is /tmp/spark/work/spark-24f6afdc-a922-4739-8875-6fdfe4f2e2d9/httpd-cce61b20-50fa-4646-ab03-537e17fe9c0d
25/12/04 11:45:14 INFO HttpServer: Starting HTTP Server
25/12/04 11:45:14 INFO Utils: Successfully started service 'HTTP file server' on port 40245.
25/12/04 11:45:14 INFO SparkContext: Added JAR file:/home/djk/Documents/shuffle-experiment/target/scala-2.10/spark-shuffle-experiment-1.0.0.jar at http://49.52.27.65:40245/jars/spark-shuffle-experiment-1.0.0.jar with timestamp 1764848714946
25/12/04 11:45:15 INFO AppClient$ClientEndpoint: Connecting to master spark://49.52.27.113:7077...
25/12/04 11:45:23 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20251204064515-0137
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor added: app-20251204064515-0137/0 on worker-20251203224515-49.52.27.60-35123 (49.52.27.60:35123) with 2 cores
25/12/04 11:45:23 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204064515-0137/0 on hostPort 49.52.27.60:35123 with 2 cores, 2.0 GB RAM
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor added: app-20251204064515-0137/1 on worker-20251203224515-49.52.27.60-35123 (49.52.27.60:35123) with 2 cores
25/12/04 11:45:23 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204064515-0137/1 on hostPort 49.52.27.60:35123 with 2 cores, 2.0 GB RAM
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor added: app-20251204064515-0137/2 on worker-20251203224515-49.52.27.60-35123 (49.52.27.60:35123) with 2 cores
25/12/04 11:45:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46533.
25/12/04 11:45:23 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204064515-0137/2 on hostPort 49.52.27.60:35123 with 2 cores, 2.0 GB RAM
25/12/04 11:45:23 INFO NettyBlockTransferService: Server created on 46533
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor added: app-20251204064515-0137/3 on worker-20251203224515-49.52.27.60-35123 (49.52.27.60:35123) with 2 cores
25/12/04 11:45:23 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204064515-0137/3 on hostPort 49.52.27.60:35123 with 2 cores, 2.0 GB RAM
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor added: app-20251204064515-0137/4 on worker-20251203144508-49.52.27.65-34725 (49.52.27.65:34725) with 2 cores
25/12/04 11:45:23 INFO BlockManagerMaster: Trying to register BlockManager
25/12/04 11:45:23 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204064515-0137/4 on hostPort 49.52.27.65:34725 with 2 cores, 2.0 GB RAM
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor added: app-20251204064515-0137/5 on worker-20251203144508-49.52.27.65-34725 (49.52.27.65:34725) with 2 cores
25/12/04 11:45:23 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204064515-0137/5 on hostPort 49.52.27.65:34725 with 2 cores, 2.0 GB RAM
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor added: app-20251204064515-0137/6 on worker-20251203144508-49.52.27.65-34725 (49.52.27.65:34725) with 2 cores
25/12/04 11:45:23 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204064515-0137/6 on hostPort 49.52.27.65:34725 with 2 cores, 2.0 GB RAM
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor added: app-20251204064515-0137/7 on worker-20251203144508-49.52.27.65-34725 (49.52.27.65:34725) with 2 cores
25/12/04 11:45:23 INFO BlockManagerMasterEndpoint: Registering block manager 49.52.27.65:46533 with 511.1 MB RAM, BlockManagerId(driver, 49.52.27.65, 46533)
25/12/04 11:45:23 INFO SparkDeploySchedulerBackend: Granted executor ID app-20251204064515-0137/7 on hostPort 49.52.27.65:34725 with 2 cores, 2.0 GB RAM
25/12/04 11:45:23 INFO BlockManagerMaster: Registered BlockManager
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor updated: app-20251204064515-0137/4 is now RUNNING
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor updated: app-20251204064515-0137/5 is now RUNNING
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor updated: app-20251204064515-0137/6 is now RUNNING
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor updated: app-20251204064515-0137/7 is now RUNNING
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor updated: app-20251204064515-0137/0 is now RUNNING
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor updated: app-20251204064515-0137/1 is now RUNNING
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor updated: app-20251204064515-0137/2 is now RUNNING
25/12/04 11:45:23 INFO AppClient$ClientEndpoint: Executor updated: app-20251204064515-0137/3 is now RUNNING
25/12/04 11:45:24 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
=== Shuffle 实验配置 ===
Manager: hash
Dataset: large
Skew:    0.0 (0=Uniform, >0=Zipf)
>>> 正在预热 (Cache & Count)...
Exception in thread "main" java.lang.IllegalStateException: SparkContext has been shutdown
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:926)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:166)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)
	at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1500)
	at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1500)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
	at org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2087)
	at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1499)
	at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1506)
	at org.apache.spark.sql.DataFrame$$anonfun$count$1.apply(DataFrame.scala:1516)
	at org.apache.spark.sql.DataFrame$$anonfun$count$1.apply(DataFrame.scala:1515)
	at org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2100)
	at org.apache.spark.sql.DataFrame.count(DataFrame.scala:1515)
	at edu.ecnu.SkewExperiment$.run(SkewExperiment.scala:43)
	at edu.ecnu.MainEntry$.main(MainEntry.scala:26)
	at edu.ecnu.MainEntry.main(MainEntry.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
25/12/04 11:45:34 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6488984103790465497, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=47]}} to 49.52.27.60/49.52.27.60:50316; closing connection
java.nio.channels.ClosedChannelException
25/12/04 11:45:34 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5178297328133958103, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=47]}} to 49.52.27.60/49.52.27.60:50310; closing connection
java.nio.channels.ClosedChannelException
25/12/04 11:45:34 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=8672488682847554980, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=47]}} to 49.52.27.60/49.52.27.60:50318; closing connection
java.nio.channels.ClosedChannelException
25/12/04 11:45:34 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6207482702941065801, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=47]}} to 49.52.27.65/49.52.27.65:41162; closing connection
java.nio.channels.ClosedChannelException
25/12/04 11:45:34 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=7607000438168238065, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=47]}} to 49.52.27.65/49.52.27.65:41176; closing connection
java.nio.channels.ClosedChannelException
25/12/04 11:45:34 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5228743822337298957, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=47]}} to 49.52.27.65/49.52.27.65:41188; closing connection
java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:470)
	at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:115)
	at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:100)
	at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254)
	at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237)
	at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644)
	at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644)
	at io.netty.channel.ChannelDuplexHandler.flush(ChannelDuplexHandler.java:117)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.access$1500(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:961)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:750)
25/12/04 11:45:34 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=6391592592256660224, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=47]}} to 49.52.27.65/49.52.27.65:41194; closing connection
java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:470)
	at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:115)
	at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:100)
	at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254)
	at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237)
	at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644)
	at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644)
	at io.netty.channel.ChannelDuplexHandler.flush(ChannelDuplexHandler.java:117)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.access$1500(AbstractChannelHandlerContext.java:32)
	at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write(AbstractChannelHandlerContext.java:961)
	at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run(AbstractChannelHandlerContext.java:893)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:750)
