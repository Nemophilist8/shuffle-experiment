#!/bin/bash
# shuffle_file_test.sh - æµ‹è¯•Hashå’ŒSort Shuffleçš„ä¸´æ—¶æ–‡ä»¶æ•°é‡å·®å¼‚

set -e

JAR_PATH="target/scala-2.10/spark-shuffle-experiment-1.0.0.jar"
MASTER="spark://49.52.27.113:7077"
TEST_DIR="logs/shuffle_file_test_$(date '+%Y%m%d_%H%M%S')"
# TEST_DIR="logs/shuffle_file_test_20251204_044258"
mkdir -p $TEST_DIR

echo "=== Shuffleä¸´æ—¶æ–‡ä»¶æ•°é‡å¯¹æ¯”æµ‹è¯• ==="
echo "æµ‹è¯•ç›®å½•: $TEST_DIR"
echo "æµ‹è¯•ç›®æ ‡: æ¯”è¾ƒHashå’ŒSort Shuffleäº§ç”Ÿçš„ä¸´æ—¶æ–‡ä»¶æ•°é‡"
echo ""

# è·å–Shuffleä¸´æ—¶æ–‡ä»¶æ•°é‡
get_shuffle_file_count() {
    local log_file=$1
    local mode=$2
    local size=$3  # <--- æ–°å¢å‚æ•°: æ•°æ®é›†å¤§å°
    
    # echo "æ­£åœ¨åˆ†æ [${mode} - ${size}] æ—¥å¿—..."
    
    # 1. æå–è€—æ—¶
    local duration=$(grep "æ‰§è¡Œè€—æ—¶:" "$log_file" | awk -F': ' '{print $2}' | sed 's/ms//')
    
    # 2. æå–æ•°æ®é‡
    local shuffle_bytes=$(grep "Shuffleæ•°æ®é‡:" "$log_file" | awk -F': ' '{print $2}')
    
    # 3. æå–æ ¸å¿ƒæŒ‡æ ‡ï¼šå³°å€¼æ–‡ä»¶æ•°
    local file_count=$(grep "å³°å€¼Shuffleæ–‡ä»¶æ•°:" "$log_file" | awk -F': ' '{print $2}' | sed 's/ ä¸ª//')
    
    # 4. æå–å³°å€¼å¤§å°
    local file_size=$(grep "å³°å€¼Shuffleæ–‡ä»¶å¤§å°:" "$log_file" | awk -F': ' '{print $2}')
    
    # echo "  > æ¨¡å¼: $mode ($size)"
    # echo "  > è€—æ—¶: ${duration}ms"
    # echo "  > æ–‡ä»¶æ•°: ${file_count}"
    # echo "  > æ€»å¤§å°: ${file_size}"

    # è¿”å›CSVæ ¼å¼æ•°æ®: æ¨¡å¼,æ•°æ®é›†,è€—æ—¶,æ–‡ä»¶æ•°,æ•°æ®é‡
    echo "$mode,$size,${duration},${file_count},${shuffle_bytes}"
}

# æ‰§è¡Œå•æ¬¡Shuffleæµ‹è¯•
run_shuffle_test() {
    local mode=$1
    local size=$2
    local description="$3 ($size)"

    # ğŸ”¥ 1. æ¯æ¬¡è¿è¡Œå‰æ¸…ç†ç¯å¢ƒï¼Œé˜²æ­¢ä¸Šä¸€è½®çš„æ–‡ä»¶å¹²æ‰°ç»Ÿè®¡
    echo ">>> [å‡†å¤‡] æ¸…ç†ç¯å¢ƒ (rm -rf /tmp/spark/work/*) ..."
    rm -rf /tmp/spark/work/*
    sleep 2
    
    echo "--- æµ‹è¯•å¼€å§‹: ${description} ---"
    
    # ğŸ”¥ 2. ä¿®æ”¹æ—¥å¿—æ–‡ä»¶åï¼ŒåŒ…å« mode å’Œ size
    local log_file="${TEST_DIR}/shuffle_${mode}_${size}.log"
    local start_time=$(date +%s)
    
    echo "æ—¥å¿—è·¯å¾„: $log_file"
    echo "æ‰§è¡Œå‚æ•°: mode=${mode}, size=${size}"
    
    # æ‰§è¡ŒSparkä½œä¸š
    spark-submit \
        --class edu.ecnu.MainEntry \
        --master $MASTER \
        --deploy-mode client \
        --executor-memory 8G \
        --driver-memory 512M \
        --executor-cores 2 \
        --conf spark.executor.instances=2 \
        --conf spark.dynamicAllocation.enabled=false \
        --total-executor-cores 4 \
        --conf spark.sql.adaptive.enabled=false \
        --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
        --conf spark.shuffle.manager=${mode} \
        --conf spark.eventLog.enabled=true \
        --conf spark.eventLog.dir="${TEST_DIR}/events_${mode}_${size}" \
        --conf spark.shuffle.service.enabled=false \
        --conf spark.shuffle.consolidateFiles=false \
        --conf spark.shuffle.sort.bypassMergeThreshold=1 \
        $JAR_PATH "file" "$mode" "$size" 2>&1 | tee "$log_file"
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    echo "ä½œä¸šæ‰§è¡Œæ—¶é—´: ${duration}ç§’"
    echo "æµ‹è¯•ç»“æŸ: ${description}"
    echo ""
    
    # ç­‰å¾…èµ„æºé‡Šæ”¾
    sleep 5
}

# === å¾ªç¯æµ‹è¯• ===

# small-x small medium large

echo "=== æµ‹è¯•é˜¶æ®µ 1: Hash Shuffle ==="
for size in medium; do
    run_shuffle_test "hash" "$size" "Hash Shuffle"
done

echo "=== æµ‹è¯•é˜¶æ®µ 2: Sort Shuffle ==="
for size in medium; do
    run_shuffle_test "sort" "$size" "Sort Shuffle"
done

# === ç»“æœåˆ†æ ===
echo "=== æµ‹è¯•ç»“æœåˆ†æ ==="

# åˆ›å»ºç»“æœæ±‡æ€»æ–‡ä»¶
result_file="${TEST_DIR}/shuffle_comparison.csv"
# ğŸ”¥ 3. CSV Header å¢åŠ  "æ•°æ®é›†" åˆ—
echo "æ¨¡å¼,æ•°æ®é›†,è€—æ—¶(ms),å³°å€¼æ–‡ä»¶æ•°,Shuffleæ•°æ®é‡" > "$result_file"

# ğŸ”¥ 4. åŒå±‚å¾ªç¯éå†æ‰€æœ‰ 6 ä¸ªæ—¥å¿—æ–‡ä»¶
for mode in hash sort; do
    for size in small medium large; do
        log_file="${TEST_DIR}/shuffle_${mode}_${size}.log"
        if [ -f "$log_file" ]; then
            # ä¼ å…¥ size å‚æ•°
            get_shuffle_file_count "$log_file" "$mode" "$size" >> "$result_file"
        else
            echo "è­¦å‘Š: æ—¥å¿—æ–‡ä»¶ $log_file ä¸å­˜åœ¨ (å¯èƒ½è¯¥ä»»åŠ¡è¿è¡Œå¤±è´¥)"
        fi
    done
done

# # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
# report_file="${TEST_DIR}/shuffle_file_analysis.md"
# cat > "$report_file" << EOF
# # Shuffleä¸´æ—¶æ–‡ä»¶æ•°é‡æµ‹è¯•æŠ¥å‘Š

# ## æµ‹è¯•æ¦‚è¿°
# - æµ‹è¯•æ—¶é—´: $(date)
# - æµ‹è¯•ç›®æ ‡: æ¯”è¾ƒä¸åŒShuffleç®¡ç†å™¨äº§ç”Ÿçš„ä¸´æ—¶æ–‡ä»¶æ•°é‡
# - æµ‹è¯•ç›®å½•: ${TEST_DIR}

# ## æ€§èƒ½æ•°æ®æ±‡æ€»
# \`\`\`csv
# $(cat "$result_file")
# \`\`\`

# ## ç»“æœåˆ†æ

# ### 1. Hash Shuffle (Spark 1.x Legacy)
# - **æœºåˆ¶**: ä¸ºæ¯ä¸ª Reduce åˆ†åŒºåˆ›å»ºä¸€ä¸ªç‹¬ç«‹æ–‡ä»¶ã€‚
# - **æ–‡ä»¶æ•°å…¬å¼**: Mapä»»åŠ¡æ•° Ã— Reduceåˆ†åŒºæ•°
# - **æœ¬æ¬¡å®éªŒè§‚å¯Ÿ**: åº”è¯¥èƒ½çœ‹åˆ°æ–‡ä»¶æ•°é‡éš Map åˆ†åŒºæ•°å¢åŠ è€Œå‰§çƒˆå¢åŠ ï¼ˆè¾¾åˆ°æ•°åƒä¸ªï¼‰ã€‚
# - **ç¼ºç‚¹**: äº§ç”Ÿæµ·é‡éšæœºå°IOï¼Œå®¹æ˜“è€—å°½ Inodeï¼Œå†…å­˜æ¶ˆè€—å¤§ï¼ˆBufferå¤šï¼‰ã€‚

# ### 2. Sort Shuffle (Standard)
# - **æœºåˆ¶**: æ¯ä¸ª Map ä»»åŠ¡åªäº§ç”Ÿ 1 ä¸ªæ•°æ®æ–‡ä»¶å’Œ 1 ä¸ªç´¢å¼•æ–‡ä»¶ã€‚
# - **æ–‡ä»¶æ•°å…¬å¼**: 2 Ã— Mapä»»åŠ¡æ•°
# - **æœ¬æ¬¡å®éªŒè§‚å¯Ÿ**: æ— è®º Reduce åˆ†åŒºå¤šå°‘ï¼Œæ–‡ä»¶æ•°éƒ½å¾ˆå°‘ï¼ˆé€šå¸¸å‡ åä¸ªï¼‰ã€‚
# - **ä¼˜ç‚¹**: é¡ºåºIOï¼Œæ–‡ä»¶ç®¡ç†é«˜æ•ˆï¼Œé€‚åˆå¤§è§„æ¨¡é›†ç¾¤ã€‚

# EOF

# echo "=== å…¨éƒ¨æµ‹è¯•å®Œæˆ ==="
# echo "æ±‡æ€»CSV: $result_file"
# echo "åˆ†ææŠ¥å‘Š: $report_file"
# echo "æ—¥å¿—ç›®å½•: $TEST_DIR"
