package edu.ecnu

import org.apache.spark.sql.{SQLContext, DataFrame}
import org.apache.spark.{SparkContext, SparkConf, SparkEnv}
import org.apache.spark.scheduler._
import java.io.File
import java.util.concurrent.Executors
import java.util.concurrent.TimeUnit
import scala.concurrent.{ExecutionContext, Future}
import scala.util.{Try, Success, Failure}
import java.util.concurrent.atomic.AtomicInteger
import java.util.UUID

object FileCountMonitor {

  def run(args: Array[String]): Unit = {
    println("Running File Count Monitor with args: " + args.mkString(" "))
    val mode = if (args.length > 0) args(0) else "sort"
    val datasetSize = if (args.length > 1) args(1) else "small"
    
    val conf = new SparkConf()
      .setAppName(s"Shuffle-File-Monitor-$mode")
      .set("spark.sql.adaptive.enabled", "false")
      .set("spark.shuffle.compress", "false")
      .set("spark.shuffle.spill.compress", "true")
      // å¢åŠ ç¼“å†²åŒºï¼Œè®© Sort Shuffle æ›´æœ‰ä¼˜åŠ¿
      .set("spark.shuffle.file.buffer", "64k")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      // è®¾ç½®æœ¬åœ°ç›®å½•ä»¥ä¾¿ç›‘æ§
      .set("spark.local.dir", "/tmp/spark/work")
      .set("spark.eventLog.enabled", "true")
      .set("spark.eventLog.dir", "/tmp/spark-events")

    // val env = SparkEnv.get
    // println(s"ğŸ”¥ğŸ”¥ğŸ”¥ å®é™…ç”Ÿæ•ˆçš„ Shuffle Manager ç±»: ${env.shuffleManager.getClass.getCanonicalName}")

    val sc = new SparkContext(conf)
    try {
      val sqlContext = new SQLContext(sc)
      sc.setLogLevel("WARN")

      println(s"=== Shuffle File Monitor å®éªŒ ===")
      println(s"3. Sparkç‰ˆæœ¬: ${sc.version}")
      println(s"é»˜è®¤shuffleåˆ†åŒºæ•°: ${sc.getConf.get("spark.sql.shuffle.partitions", "200")}")
      println(s"è¯·æ±‚çš„Shuffle Manager: $mode")
      println(s"å½“å‰ Shuffle Manager: ${sc.getConf.get("spark.shuffle.manager", "sort")}")
      println(s"Spark æœ¬åœ°ç›®å½•: ${sc.getConf.get("spark.local.dir", "/tmp/spark/work")}")
      println(s"ç›‘æ§å°†æ¯ç§’æ£€æŸ¥ä¸€æ¬¡ /tmp/spark/work ç›®å½•ä¸­çš„ shuffle_* æ–‡ä»¶")
      
      // æ¸…ç†æ—§çš„ç›‘æ§ç›®å½•ï¼ˆå¯é€‰ï¼‰
      // val hadoopConf = sc.hadoopConfiguration
      // val fs = org.apache.hadoop.fs.FileSystem.get(hadoopConf)


      // åªç”ŸæˆæŒ‡å®šçš„ä¸€ä¸ªæ•°æ®é›†
      val df = datasetSize match {
        case "small-x" => DataGenerator.generateUniform(sqlContext, "small-x")
        case "small" => DataGenerator.generateUniform(sqlContext, "small")
        case "medium" => DataGenerator.generateUniform(sqlContext, "medium")
        case "large" => DataGenerator.generateUniform(sqlContext, "large")
      }

      // djkæµ‹è¯•å€¾æ–œæ•°æ®
      // val df = datasetSize match {
      //   case "small-x" => DataGenerator.generateSkewed(sqlContext, "small-x")
      //   case "small" => DataGenerator.generateSkewed(sqlContext, "small")
      //   case "medium" => DataGenerator.generateSkewed(sqlContext, "medium")
      //   case "large" => DataGenerator.generateSkewed(sqlContext, "large")
      // }

      println(s"æµ‹è¯•æ•°æ®é›†: $datasetSize")
      // println(s"æµ‹è¯•æ•°æ®é›†V2: $size, è®°å½•æ•°: ${df.count()}")
      println(s"åˆ†åŒºæ•°: ${df.rdd.getNumPartitions}")

      // ==========================================
      // æ–°å¢ï¼šé¢„çƒ­é˜¶æ®µ (è§£å†³ 0 æ–‡ä»¶ç­‰å¾…é—®é¢˜)
      // ==========================================
      println(">>> æ­£åœ¨è¿›è¡Œæ•°æ®é¢„çƒ­ (Cache & Count)...")
      // 1. å°†æ•°æ®ç¼“å­˜åˆ°å†…å­˜
      df.cache() 
      // 2. å¼ºåˆ¶è§¦å‘ä¸€æ¬¡è®¡ç®—ï¼Œè®© Executors æŠŠæ•°æ®çœŸæ­£åœ¨å†…å­˜é‡Œç”Ÿæˆå¥½
      val warmupStart = System.currentTimeMillis()
      val count = df.count() 
      val warmupEnd = System.currentTimeMillis()
      println(s">>> é¢„çƒ­å®Œæˆ! æ•°æ®å·²é©»ç•™å†…å­˜ã€‚è€—æ—¶: ${warmupEnd - warmupStart}ms, è®°å½•æ•°: $count")
      println(">>> ç°åœ¨å¼€å§‹æ­£å¼ Shuffle å®éªŒ (æ­¤æ—¶æ–‡ä»¶åº”è¯¥ä¼šç«‹å³äº§ç”Ÿ)")

      val (time, bytes, fileCount, fileSizeKB) = runExperiment(df, sc, mode)
        
      println(s"\n" + "="*50)
      println(s"æµ‹è¯•ç»“æœ [$mode]:")
      println(s"  æ‰§è¡Œè€—æ—¶: ${time}ms")
      println(s"  Shuffleæ•°æ®é‡: ${formatBytes(bytes)}")
      println(s"  å³°å€¼Shuffleæ–‡ä»¶æ•°: $fileCount ä¸ª")
      println(s"  å³°å€¼Shuffleæ–‡ä»¶å¤§å°: ${fileSizeKB}KB (${"%.2f".format(fileSizeKB/1024.0)}MB)")
      println(s"  Shuffleç±»å‹åˆ†æ: ${analyzeShuffleType(fileCount, fileSizeKB)}")
      
      // ç”Ÿæˆæ–‡ä»¶åˆ†ææŠ¥å‘Š
      if (fileCount > 0) {
        val avgFileSizeKB = if (fileCount > 0) fileSizeKB / fileCount else 0
        println(s"  å¹³å‡æ¯ä¸ªæ–‡ä»¶å¤§å°: ${avgFileSizeKB}KB")
        
        // æ ¹æ®æ–‡ä»¶ç‰¹å¾ç»™å‡ºå»ºè®®
        if (fileCount > 500) {
          println(s"  âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°å¤§é‡å°æ–‡ä»¶($fileCount)ä¸ªï¼Œå»ºè®®ä½¿ç”¨Sort Shuffle")
        } else if (avgFileSizeKB < 100 && fileCount > 100) {
          println(s"  â„¹ï¸  æç¤º: è¾ƒå¤šå°æ–‡ä»¶ï¼Œè€ƒè™‘è°ƒæ•´spark.shuffle.file.buffer")
        }
      }
    } finally {
      sc.stop()
    }
  }
  
  def runExperiment(df: DataFrame, sc: SparkContext, name: String): (Long, Long, Int, Long) = {
    val listener = new FileCountMonitorMetricsListener
    sc.addSparkListener(listener)
    
    // å¯åŠ¨æ–‡ä»¶ç›‘æ§
    val monitor = new MyShuffleFileMonitor()
    println(s"\n=== å¼€å§‹è¿è¡Œ $name å®éªŒ ===")
    println(s"å¯åŠ¨Shuffleæ–‡ä»¶ç›‘æ§ï¼Œæ¯ç§’æ£€æŸ¥ä¸€æ¬¡...")
    monitor.startMonitoring(intervalSeconds = 1, durationSeconds = 180)
    
    val startTime = System.currentTimeMillis()
    
    // æ‰§è¡ŒShuffleæ“ä½œ

    // å°ä»»åŠ¡é…ç½®è¾ƒå°‘çš„åˆ†åŒº
    // val shufflePartitions = 100

    // å¤§ä»»åŠ¡é…ç½®è¾ƒå¤šçš„åˆ†åŒº
    val shufflePartitions = 500
    
    // DataGenerator ç”Ÿæˆçš„åˆ—: 0:key, 1:value, 2:category, 3:payload
    val result = df.rdd
      .map { row => 
        (row.getString(0), row.getString(3)) 
      }
      .groupByKey(shufflePartitions)
      .count() // è§¦å‘ Action
    
    val endTime = System.currentTimeMillis()
    Thread.sleep(5000)

    val shuffleBytes = listener.getShuffleWriteBytes
    
    monitor.stopMonitoring()
    (endTime - startTime, shuffleBytes, monitor.getMaxFileCount, monitor.getMaxTotalSizeKB)
  }
  
  def formatBytes(bytes: Long): String = {
    val units = Array("B", "KB", "MB", "GB", "TB")
    if (bytes <= 0) return "0 B"
    val digitGroups = (Math.log10(bytes.toDouble) / Math.log10(1024)).toInt
    val unit = units(Math.min(digitGroups, units.length - 1))
    val value = bytes / Math.pow(1024, Math.min(digitGroups, units.length - 1))
    f"$value%.2f $unit"
  }
  
  def analyzeShuffleType(fileCount: Int, fileSizeKB: Long): String = {
    if (fileCount > 1000) s"Hash Shuffle (æ£€æµ‹åˆ°å¤§é‡å°æ–‡ä»¶: $fileCount ä¸ª)"
    else if (fileCount <= 200) s"Sort Shuffle (æ–‡ä»¶é«˜åº¦åˆå¹¶: $fileCount ä¸ª)"
    else s"æ··åˆ/ä¸ç¡®å®š ($fileCount ä¸ªæ–‡ä»¶)"
  }

  
}

class MyShuffleFileMonitor(sparkLocalDir: String = "/tmp/spark/work") {
  private val executorService = Executors.newSingleThreadExecutor()
  private var isMonitoring = false
  private val maxFileCount = new AtomicInteger(0)
  private var maxTotalSize: Long = 0L
  
  def startMonitoring(intervalSeconds: Int = 1, durationSeconds: Int = 300): Unit = {
    isMonitoring = true
    val startTime = System.currentTimeMillis()
    
    executorService.submit(new Runnable {
      override def run(): Unit = {
        while (isMonitoring && 
               System.currentTimeMillis() - startTime < durationSeconds * 1000) {
          
          try {
            // è·å–å½“å‰æ—¶é—´æˆ³
            val timestamp = new java.text.SimpleDateFormat("HH:mm:ss")
              .format(new java.util.Date())
            
            // æŸ¥æ‰¾shuffleæ–‡ä»¶
            val shuffleDir = new File(sparkLocalDir)
            val (fileCount, totalSizeKB) = if (shuffleDir.exists() && shuffleDir.isDirectory) {
              countShuffleFiles(shuffleDir)
            } else {
              (0, 0L)
            }
            
            // æ›´æ–°æœ€å¤§å€¼
            if (fileCount > maxFileCount.get()) {
              maxFileCount.set(fileCount)
            }
            if (totalSizeKB > maxTotalSize) {
              maxTotalSize = totalSizeKB
            }
            
            // è¾“å‡ºç›‘æ§ä¿¡æ¯
            println(s"[ShuffleMonitor $timestamp] æ–‡ä»¶æ•°: $fileCount, " +
                   s"æ€»å¤§å°: ${formatSizeKB(totalSizeKB)} (å³°å€¼: ${maxFileCount.get()}æ–‡ä»¶, ${formatSizeKB(maxTotalSize)})")
            
            // å¦‚æœå‘ç°æ–‡ä»¶æ•°é‡å¼‚å¸¸å¤šï¼Œå¯èƒ½æ˜¯Hash Shuffleçš„ç‰¹å¾
            if (fileCount > 1000) {
              println(s"[ShuffleMonitor è­¦å‘Š] æ£€æµ‹åˆ°å¤§é‡ä¸´æ—¶æ–‡ä»¶($fileCount ä¸ª)ï¼Œè¿™å¯èƒ½æ˜¯Hash Shuffleçš„ç‰¹å¾")
            }
            
            // ç­‰å¾…æŒ‡å®šé—´éš”
            Thread.sleep(intervalSeconds * 1000)
          } catch {
            case e: Exception =>
              println(s"[ShuffleMonitor é”™è¯¯] ${e.getMessage}")
          }
        }
      }
    })
  }
  
  private def countShuffleFiles(directory: File): (Int, Long) = {
    var fileCount = 0
    var totalSizeBytes: Long = 0L
    
    def scanDir(dir: File): Unit = {
      if (dir.exists() && dir.isDirectory) {
        val files = dir.listFiles()
        if (files != null) {
          files.foreach { file =>
            if (file.isFile && file.getName.startsWith("shuffle_")) {
              fileCount += 1
              totalSizeBytes += file.length()
            } else if (file.isDirectory) {
              scanDir(file)
            }
          }
        }
      }
    }
    
    scanDir(directory)
    val totalSizeKB = totalSizeBytes / 1024
    (fileCount, totalSizeKB)
  }
  
  private def formatSizeKB(kb: Long): String = {
    if (kb < 1024) s"${kb}KB"
    else if (kb < 1024 * 1024) f"${kb / 1024.0}%.2fMB"
    else f"${kb / (1024.0 * 1024.0)}%.2fGB"
  }
  
  def stopMonitoring(): Unit = {
    isMonitoring = false
    executorService.shutdown()
    
    println(s"\n[ShuffleMonitor æœ€ç»ˆæŠ¥å‘Š]")
    println(s"å³°å€¼æ–‡ä»¶æ•°: ${maxFileCount.get()}")
    println(s"å³°å€¼æ–‡ä»¶å¤§å°: ${formatSizeKB(maxTotalSize)}")
    
    // æ ¹æ®æ–‡ä»¶æ•°é‡åˆ¤æ–­Shuffleç±»å‹
    val estimatedType = if (maxFileCount.get() > 500) "Hash Shuffleï¼ˆæ–‡ä»¶æ•°>500ï¼‰" else "Sort Shuffle"
    println(s"æ¨æµ‹Shuffleç±»å‹: $estimatedType")
  }
  
  def getMaxFileCount: Int = maxFileCount.get()
  def getMaxTotalSizeKB: Long = maxTotalSize
}

class FileCountMonitorMetricsListener extends SparkListener {
  private var shuffleWriteBytes: Long = 0L
  
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    val metrics = taskEnd.taskMetrics
    if (metrics != null) {
      metrics.shuffleWriteMetrics.foreach { writeMetrics =>
        shuffleWriteBytes += writeMetrics.shuffleBytesWritten
      }
    }
  }
  
  def getShuffleWriteBytes: Long = shuffleWriteBytes
  def reset(): Unit = shuffleWriteBytes = 0L
}